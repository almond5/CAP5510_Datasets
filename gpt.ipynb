{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (4.45.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: filelock in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.1.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: regex in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sacremoses) (2024.9.11)\n",
      "Requirement already satisfied: click in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sacremoses) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sacremoses) (1.4.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sacremoses) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from click->sacremoses) (0.4.6)\n",
      "Requirement already satisfied: torch in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (3.10.9)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (0.25.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp->datasets) (1.13.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests>=2.32.2->datasets) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests>=2.32.2->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests>=2.32.2->datasets) (2022.9.24)\n",
      "Requirement already satisfied: colorama in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\almond\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install sacremoses\n",
    "%pip install torch\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BioGptForCausalLM(\n",
       "  (biogpt): BioGptModel(\n",
       "    (embed_tokens): BioGptScaledWordEmbedding(42384, 1024, padding_idx=1)\n",
       "    (embed_positions): BioGptLearnedPositionalEmbedding(1026, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x BioGptDecoderLayer(\n",
       "        (self_attn): BioGptSdpaAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (activation_fn): GELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (output_projection): Linear(in_features=1024, out_features=42384, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "model = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Osimertinib is used as the first-line treatmen...\n",
      "1     We wanted to present a rare case of metastatic...\n",
      "2     Chemotherapy in an integral part of cancer tre...\n",
      "3     DNA methylation plays a regulatory role in the...\n",
      "4     Lung cancer is the leading cause of cancer-rel...\n",
      "5     Diffuse large B-cell lymphoma is the most comm...\n",
      "6     To evaluate the efficacy and safety of program...\n",
      "7     Breast cancer (BC) is the most frequently occu...\n",
      "8     Breast cancer has the potential to metastasize...\n",
      "9     Bronchiolitis obliterans syndrome (BOS) occurr...\n",
      "10    For drugs with a narrow therapeutic window, th...\n",
      "11    The IROC head and neck phantom is used to cred...\n",
      "12    To compare the efficacy of first-line regimens...\n",
      "13    This systematic review and network meta-analys...\n",
      "14    Accurate staging improves lung cancer survival...\n",
      "15    This research aimed to systematically uncover ...\n",
      "16    Lymph node metastasis (LNM) plays a crucial ro...\n",
      "17    Lung cancer is a leading cause of cancer-relat...\n",
      "18    Durvalumab is an IgG1 monoclonal antibody that...\n",
      "19    The advent of immune checkpoint inhibitors (IC...\n",
      "20    Small cell lung cancer (SCLC) is a highly inva...\n",
      "21    With the advent of targeted therapies, the sur...\n",
      "22    A dosimetric evaluation is still lacking in te...\n",
      "23    Nutritional status assessment has been deemed ...\n",
      "24    Ranked high in worldwide growing health issues...\n",
      "25    The optimal number of lymph nodes to be dissec...\n",
      "26    Carinal reconstruction remains a technically c...\n",
      "27    Amivantamab is an epidermal growth factor rece...\n",
      "28    The risk of treatment-related toxicities with ...\n",
      "29    Surgical resection is the best-known approach ...\n",
      "30    Being implicated in bone metastasis developmen...\n",
      "31    To explore how non-surgical esophageal cancer ...\n",
      "32    Epidermal growth factor receptor (EGFR) tyrosi...\n",
      "33    Lung cancer in Australia contributes 9% of all...\n",
      "34    Breast tissue markers are essential in localis...\n",
      "35    Colorectal cancer is the 3rd most common cance...\n",
      "36    Epidermal growth factor receptor (EGFR) kinase...\n",
      "37    At JADPRO Live 2023, presenters discussed the ...\n",
      "38    Dishevelled, EGL-10 and pleckstrin domain-cont...\n",
      "39    Ga-68 labeled prostate-specific membrane antig...\n",
      "40    Direct comparisons between 18 Fifty-one patien...\n",
      "41    This was a multicenter, single-arm dose-rangin...\n",
      "42    Cutaneous malignant melanoma is one of the mos...\n",
      "43    The effect of cancer in women has varied effec...\n",
      "44    Capmatinib is a selective MET inhibitor with d...\n",
      "45    As more treatments emerge for advanced, stage ...\n",
      "46    Immunotherapy has revolutionized the managemen...\n",
      "47    The Onco Summit 2023: The Latin American (LATA...\n",
      "48    The advent of immunotherapy has greatly improv...\n",
      "49    Antrodia cinnamomea-derived sulfated polysacch...\n",
      "Name: Abstract, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 243.54 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 3697.70 examples/s]\n",
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [05:40<00:00, 16.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 340.2966, 'train_samples_per_second': 0.264, 'train_steps_per_second': 0.062, 'train_loss': 2.0035483950660344, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation: {'eval_loss': 2.1885786056518555, 'eval_runtime': 13.9675, 'eval_samples_per_second': 0.716, 'eval_steps_per_second': 0.143, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=1024)\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "df = pd.read_csv('Question_1.csv') \n",
    "\n",
    "# Extract the abstracts\n",
    "abstracts = df['Abstract']\n",
    "abstracts = abstracts[:50]\n",
    "print(abstracts)\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": abstracts})\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "input_ids = tokenized_dataset['input_ids']\n",
    "labels = input_ids.clone()\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)\n",
    "\n",
    "train_size = 30\n",
    "val_size = 10\n",
    "test_size = 10\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    tokenized_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Update Trainer with train and validation sets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,    # Training set\n",
    "    eval_dataset=val_dataset,       # Validation set for tuning during training\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test set evaluation:\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x00000156B93948E0>\n",
      "Treatment for lung cancer includes surgery, radiation therapy and chemotherapy. The role of radiation in the management of non-small-cell lung carcinoma. A multidisciplinary approach. Treatment strategies, the role and the value of the radiation oncologist. This article reviews the current state of lung-cancer treatment and discusses the indications, clinical role, and value for radiation oncologists. Our purpose is to provide a framework for multidisciplinary team (MDT) management for patients with lung cancers. We present a comprehensive review of treatment for non small cell carcinoma (NSCLC) in an MDT setting. Lung-care is a multidisciplinary effort that encompasses the patient, family, oncology,\n",
      "Treatment for lung cancer includes chemotherapy, radiation, and targeted therapy. The first-line treatment options for non-small-cell lung carcinoma include cisplatin-based doublet chemotherapy and platinum-free doublet therapy (i.e., carboplatin plus paclitaxel). Lung cancer is the leading cause of cancer-related deaths worldwide. There is a great need to develop new treatment strategies for patients with non small cell carcinoma. A number of targeted therapies have been approved for the treatment of non non malignant lung disease, including tyrosine kinase inhibitors for human epidermal growth factor receptor 2 (HER2) -positive and anaplastic lymphoma kinase (ALK) -rearranged lung cancers. These targeted agents have\n",
      "Treatment for lung cancer includes radiation therapy, chemotherapy, and targeted therapy. The current standard of care for patients with unresectable stage III non-small cell lung carcinoma (NSCLC) is chemotherapy combined with radiotherapy. Treatment strategies vary widely across institutions and are generally based on best available evidence. We aimed to conduct a systematic review of the evidence for the treatment of stage IIIB non small-cell carcinoma of lung (cStage IIIB NSCLC, stage IIIC disease). A systematic literature review was performed on PubMed and EMBASE databases and the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines were followed. Inclusion criteria were: (1)\n",
      "Treatment for lung cancer includes surgical resection, radiation therapy, chemotherapy, and targeted therapies. The current standard of care is surgical therapy followed by adjuvant chemotherapy. Final results of a phase III randomized trial of surgical versus medical therapy for patients with stage IIIA non-small cell lung carcinoma (NSCLC) demonstrate that surgery is superior to chemotherapy for the treatment of stage III NSCLC, with a survival advantage of approximately 2 years. This study is registered with ClinicalTrials.gov, number NCT00190830.\n",
      "Treatment for lung cancer includes surgery, radiation therapy and chemotherapy. The role of surgery in patients with brain metastases. Lung cancer is the leading cause of cancer death in both men and women. Treatment of brain metastatic cancer: A comprehensive review. We searched Pubmed, Embase and Cochrane Database of Systematic Reviews for relevant studies. Patients with lung cancers that are diagnosed with metastasis to the brain are usually treated with surgery for brain metastasis. All studies were graded according to a modified Sackett scale.\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)\n",
    "\n",
    "prompt = \"Treatment for lung cancer includes\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  \n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_length=128,\n",
    "    num_return_sequences=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Real-world data are limited for patients with ...\n",
      "1     Risk management and self-management strategies...\n",
      "2     Postoperative pancreatic fistula (POPF) contin...\n",
      "3     We aim to develop a predictive model for lymph...\n",
      "4     Breast cancer is the most common invasive canc...\n",
      "5     The titled molecule 2-Amino-N-(2-chloro-6-meth...\n",
      "6     In resource-limited settings, data regarding t...\n",
      "7     Chemotherapy in an integral part of cancer tre...\n",
      "8     STK11 germline pathogenic variants are typical...\n",
      "9                Background Methods Results Conclusions\n",
      "10             Introduction Methods Results Conclusions\n",
      "11    The presented study depicts the synthesis of 1...\n",
      "12    Breast cancer is one of the most common cancer...\n",
      "13    Breast cancer ranks as the second most prevale...\n",
      "14    As the spatial resolution of positron emission...\n",
      "15    We aimed to investigate the impact of social c...\n",
      "16    Breast cancer (BC) is the most frequently occu...\n",
      "17    Breast Cancer (BC) poses significant challenge...\n",
      "18    Breast cancer is the most frequently diagnosed...\n",
      "19    Approximately 10 to 15% of patients with breas...\n",
      "20    Breast cancer has the potential to metastasize...\n",
      "21    One of the main regulators in the cell cycle i...\n",
      "22    A century ago, the first description of second...\n",
      "23    Progressive multifocal leukoencephalopathy (PM...\n",
      "24    A high abundance of Epidermal Growth Factor Re...\n",
      "25    In patients vulnerable to skeletal-related eve...\n",
      "26    Arytenoid dislocation, typically manifested as...\n",
      "27    Radiotherapy (RT) serves as one of the key adj...\n",
      "28    Breast cancer is one of the main Cancers affec...\n",
      "29    To find highly effective and low-toxicity anti...\n",
      "30    Background Objectives: Methods: Results: 2 2 C...\n",
      "31    Loop ileostomy is commonly performed by colore...\n",
      "32    There are limited real-world data (RWD) regard...\n",
      "33    Prophylactic dissection of paratracheal lymph ...\n",
      "34    To evaluate the efficacy and safety of four te...\n",
      "35    Cancer is a complex pathological condition ass...\n",
      "36    In the current study, we have investigated the...\n",
      "37    Globally, hormone-responsive cancers afflict m...\n",
      "38    Modern cancer therapies greatly improve clinic...\n",
      "39    Breast reconstruction after mastectomy helps w...\n",
      "40    Image-guided biopsy is an integral step in the...\n",
      "41    Surgical resection is the best-known approach ...\n",
      "42    Image-based data mining (IBDM) requires spatia...\n",
      "43    Being implicated in bone metastasis developmen...\n",
      "44    Objective: Methods: Results: P P Conclusion: Â©...\n",
      "45    This study aimed to assess the targeted nano-t...\n",
      "46    Radiation therapy plays a critical role in the...\n",
      "47    Breast tissue markers are essential in localis...\n",
      "48    There are limited options for primary preventi...\n",
      "49    Dishevelled, EGL-10 and pleckstrin domain-cont...\n",
      "Name: Abstract, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 274.74 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 4165.73 examples/s]\n",
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [06:14<00:00, 17.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 374.4316, 'train_samples_per_second': 0.24, 'train_steps_per_second': 0.056, 'train_loss': 2.340392521449498, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:23<00:00, 11.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation: {'eval_loss': 2.2494518756866455, 'eval_runtime': 23.9823, 'eval_samples_per_second': 0.417, 'eval_steps_per_second': 0.083, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=1024)\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "df = pd.read_csv('Question_2.csv') \n",
    "\n",
    "# Extract the abstracts\n",
    "abstracts = df['Abstract']\n",
    "abstracts = abstracts[:50]\n",
    "print(abstracts)\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": abstracts})\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "input_ids = tokenized_dataset['input_ids']\n",
    "labels = input_ids.clone()\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)\n",
    "\n",
    "train_size = 30\n",
    "val_size = 10\n",
    "test_size = 10\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    tokenized_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Update Trainer with train and validation sets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,    # Training set\n",
    "    eval_dataset=val_dataset,       # Validation set for tuning during training\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test set evaluation:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x00000157017E7A30>\n",
      "Treatment for breast cancer includes surgery, chemotherapy and / or radiation, and is usually administered over a course of several weeks. (1) Despite being the leading cause of cancer death in women worldwide, the management of breast cancers has evolved dramatically over the past decades. The development of targeted therapy has improved the survival of patients with breast carcinoma; however, it has also increased the risk of drug-related adverse events (ADRs), which can limit the use of these drugs and cause unnecessary side effects. In this context, understanding the causes of ADRs is crucial for the development and clinical use, as well as for improving drug safety. We conducted\n",
      "Treatment for breast cancer includes surgery, chemotherapy, and hormone therapy. An understanding of breast anatomy and the breast and its surrounding structures is essential for surgical planning. The aim of this study was to provide a comprehensive review of the literature on the anatomy of and techniques used for the treatment of benign and malignant breast diseases. This review was conducted using PubMed, EMBASE, Scopus, Web of Science, the Cochrane Library, CINAHL, Google Scholar, EBSCO, Medline Complete, Embase Classic, ERIC, Science Direct, ProQuest, Springer, Wiley, Physiotherapy Evidence Database, Cochrane Database of Systematic Reviews, SCOPUS, I\n",
      "Treatment for breast cancer includes local and / or systemic therapy. The use of systemic therapies has been increasing over the past decades; however, most of the available drugs have significant side effects. Based on the high prevalence of side-effects, it is important to identify and prioritize the most relevant side effect of each drug. We aimed to develop an algorithm to prioritize side impact of drugs used for the treatment of breast cancers. Thus, the main objective of this study was to construct a side Impact score (SIS) for each breast drug and its associated comorbidities. METHODS: We performed a literature review for side consequences of approved drugs for treatment breast-\n",
      "Treatment for breast cancer includes surgery, radiation therapy, chemotherapy, and hormonal therapy. The breast is a complex structure and may be affected by radiation and chemotherapy in combination or sequentially. As a result, the radiation treatment has to be adapted to the individual patient. This paper provides a detailed overview of the treatment planning and delivery of breast radiation, including the physical principles, dose calculation, clinical applications, treatment verification, patient positioning, image guidance, quality assurance, planning, radiotherapy delivery, normal tissue toxicity, toxicity management, as well as follow-up. In addition, this paper also provides practical advice on how to ensure that all these aspects are\n",
      "Treatment for breast cancer includes breast-conserving surgery (BCS), breast radiotherapy (RT) and endocrine therapy. The role of adjuvant systemic therapy has been controversial for decades. However, the recent introduction of targeted therapy, such as tyrosine kinase inhibitors (TKIs) for HER2-positive patients, has changed the treatment paradigm. In this review, we provide an overview of the clinical trials and the most recent data on the role and safety of systemic therapies, with a focus on TKIs. A comprehensive search was conducted using PubMed and Embase. We included clinical studies published in the English language in which systemic treatment was performed for the prevention or treatment of breast\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)\n",
    "\n",
    "prompt = \"Treatment for breast cancer includes\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  \n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_length=128,\n",
    "    num_return_sequences=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Pediatric neurosurgical practice is prevalent ...\n",
      "1     Posttraumatic osteoarthritis is a common indic...\n",
      "2     Delay in diagnosis and treatment of lung cance...\n",
      "3     Optical imaging is a powerful tool for early d...\n",
      "4     Successful acute migraine treatment potentiall...\n",
      "5     Masses in the forefoot and midfoot are common ...\n",
      "6     Acute kidney injury (AKI) is a prevalent compl...\n",
      "7     Despite the significant decline in the inciden...\n",
      "8     Neoadjuvant endocrine therapy presents an impo...\n",
      "9     Auxiliary diagnosis of different types of cyst...\n",
      "10    Osteoarthritis is more prevalent and severe am...\n",
      "11    The integration of positron emission tomograph...\n",
      "12    With the advent of PET imaging in 1976, 2-deox...\n",
      "13    Alzheimer's disease (AD) is the most common ne...\n",
      "14    Despite an increase in maternal prenatal canna...\n",
      "15    There is an unmet need for effective topical t...\n",
      "16    Attention Deficit Hyperactivity Disorder (ADHD...\n",
      "17    Gastric cancer (GC) is one of the most common ...\n",
      "18    Mucinous adenocarcinoma of anus is an uncommon...\n",
      "19    Chronotype is an inherent physiological trait ...\n",
      "20    Many consider white matter hyperintensities (W...\n",
      "21    Growing skull fracture (GSF) of the orbital ro...\n",
      "22    Cerebral aneurysms are diagnosed in 1-5% of pe...\n",
      "23    To investigate the effect of two fractionation...\n",
      "24    Exercise-induced bronchoconstriction (EIB) is ...\n",
      "25    Drug Reaction with Eosinophilia and Systemic S...\n",
      "26    Cervical cancer screening is a cornerstone of ...\n",
      "27    Cardiac amyloidosis (CA) is characterized by d...\n",
      "28    This study aimed to analyze the demographic ch...\n",
      "29    Blastomycosis is an environmentally acquired f...\n",
      "30    Acute kidney injury (AKI) and chronic kidney d...\n",
      "31    Adverse childhood experiences (ACEs) have long...\n",
      "32    Reductions in all-cause otitis media (OM) foll...\n",
      "33    Cancer will affect more than one in three U.S....\n",
      "34    Renal cell carcinoma (RCC) is a highly aggress...\n",
      "35    We demonstrate the peculiarities of diagnosis ...\n",
      "36    A rare case of thyroid foreign body is present...\n",
      "37    To create the prognostic scale based on some b...\n",
      "38    Thyroid cancer is the most prevalent form of e...\n",
      "39    In this study, we aimed to identify bone miner...\n",
      "40    The early detection and accurate monitoring of...\n",
      "41    This study aimed to analyze the prevalence and...\n",
      "42    The glaucoma treatment paradigm is starting to...\n",
      "43    Hundreds of thousands of children are victims ...\n",
      "44    Despite a higher rate of breast cancer in sub-...\n",
      "45    Psoriatic arthritis (PsA) is a heterogeneous, ...\n",
      "46    Optical coherence tomography angiography (OCTA...\n",
      "47    Surgical wound infection is a global postopera...\n",
      "48    Stroke poses a significant global health threa...\n",
      "49    Primary gastric small cell carcinoma (GSCC) is...\n",
      "Name: Abstract, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 241.58 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 3845.23 examples/s]\n",
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [07:50<00:00, 22.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 470.258, 'train_samples_per_second': 0.191, 'train_steps_per_second': 0.045, 'train_loss': 2.1990267435709634, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:15<00:00,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation: {'eval_loss': 2.397444725036621, 'eval_runtime': 15.2271, 'eval_samples_per_second': 0.657, 'eval_steps_per_second': 0.131, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=1024)\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "df = pd.read_csv('Question_3.csv') \n",
    "\n",
    "# Extract the abstracts\n",
    "abstracts = df['Abstract']\n",
    "abstracts = abstracts[:50]\n",
    "print(abstracts)\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": abstracts})\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "input_ids = tokenized_dataset['input_ids']\n",
    "labels = input_ids.clone()\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)\n",
    "\n",
    "train_size = 30\n",
    "val_size = 10\n",
    "test_size = 10\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    tokenized_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Update Trainer with train and validation sets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,    # Training set\n",
    "    eval_dataset=val_dataset,       # Validation set for tuning during training\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test set evaluation:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x000001571816AE60>\n",
      "Symptoms for lung cancer include cough, dyspnea, hemoptysis, chest pain, and chest tightness. The purpose of this study was to investigate the impact of lung-cancer-specific symptoms on quality of life (QOL) of patients and caregivers. A survey was conducted among patients with lung cancers and their caregivers in China. Patients were administered questionnaires to collect socio-demographic data and lung symptoms, including cough and dyspnea. Caregivers were also administered the same questionnaire. QOL was evaluated using the lung Cancer Symptom Scale (LCSS). Multiple regression analysis was used to identify factors that influence patients' QOL and caregiver QOL. Results: A total of 200\n",
      "Symptoms for lung cancer include dyspnea, chest pain, cough, and hemoptysis. The diagnosis of lung carcinoma typically requires advanced imaging techniques, such as computed tomography, which are usually performed in combination with tissue biopsy. Lung cancer can be detected in early stages, but imaging often lacks specificity. Therefore, the diagnostic accuracy of noninvasive techniques for early lung malignancy is limited. Herein, we present the case of a 79-year-old female patient with cough and dyspnea of two months' duration. Chest imaging revealed a mass in the right lower lobe (LLL). Surgical resection was performed and histopathological examination of the LLM revealed lung adenocarcinoma. Immunohistochemical\n",
      "Symptoms for lung cancer include cough, chest pain, hemoptysis, dyspnea, and cough-induced fever. The diagnosis of lung cancers requires a high degree of clinical suspicion and is often difficult to confirm. Early diagnosis is essential to improve the prognosis of patients. Here, we report the case of a patient with lung adenocarcinoma presenting as a persistent cough and a history of chest trauma. He was successfully treated with a combination of surgical resection and chemotherapy. To the best of our knowledge, this is the first report of an association between lung carcinoma and chest injury. Our findings suggest that clinicians should be vigilant for the possibility of underlying chest injuries in\n",
      "Symptoms for lung cancer include cough, shortness of breath, chest pain, hemoptysis, and chest tightness. The diagnosis of lung adenocarcinoma is often difficult, but the detection of specific proteins in blood can assist in the diagnosis. Lung cancer biomarkers are useful for diagnosis and early treatment. Blood biomarkers can also be used to assess the efficacy of therapy and monitor disease progression. However, current blood biomarkers for cancer have limitations, such as short half-life, poor specificity, or lack of sensitivity. To address these problems, several studies have attempted to develop blood-based biomarkers to improve early diagnosis, monitoring, treatment, prognosis, drug development, therapeutic\n",
      "Symptoms for lung cancer include cough, chest pain, shortness of breath, hemoptysis, and fever. The diagnosis of lung adenocarcinoma is usually confirmed based on histology, but other types of cancer can also mimic symptoms. However, the diagnostic workup is often challenging because of the similarities between symptoms of both diseases. Therefore, differential diagnosis is crucial for appropriate diagnosis. This article aims to provide an overview of common symptoms, diagnostic work-up, imaging, treatment, prognosis, follow-ups, or prognosis of patients with lung adenocarcinomas. Lung cancer is a heterogeneous disease, with symptoms ranging from benign to life-threatening. In the early stages, patients present\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)\n",
    "\n",
    "prompt = \"Symptoms for lung cancer include\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  \n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_length=128,\n",
    "    num_return_sequences=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Pediatric neurosurgical practice is prevalent ...\n",
      "1     Posttraumatic osteoarthritis is a common indic...\n",
      "2     Delay in diagnosis and treatment of lung cance...\n",
      "3     Paclitaxel-induced peripheral neuropathy (PN) ...\n",
      "4     Breast cancer is the leading cause of cancer-r...\n",
      "5     Optical imaging is a powerful tool for early d...\n",
      "6     The study objective was to determine the effec...\n",
      "7     Successful acute migraine treatment potentiall...\n",
      "8     Masses in the forefoot and midfoot are common ...\n",
      "9     Acute kidney injury (AKI) is a prevalent compl...\n",
      "10    Despite the significant decline in the inciden...\n",
      "11    Neoadjuvant endocrine therapy presents an impo...\n",
      "12    Auxiliary diagnosis of different types of cyst...\n",
      "13    Osteoarthritis is more prevalent and severe am...\n",
      "14    The integration of positron emission tomograph...\n",
      "15    With the advent of PET imaging in 1976, 2-deox...\n",
      "16    Alzheimer's disease (AD) is the most common ne...\n",
      "17    Despite an increase in maternal prenatal canna...\n",
      "18    There is an unmet need for effective topical t...\n",
      "19    Attention Deficit Hyperactivity Disorder (ADHD...\n",
      "20    Gastric cancer (GC) is one of the most common ...\n",
      "21    Mucinous adenocarcinoma of anus is an uncommon...\n",
      "22    Chronotype is an inherent physiological trait ...\n",
      "23    Many consider white matter hyperintensities (W...\n",
      "24    Growing skull fracture (GSF) of the orbital ro...\n",
      "25    Cerebral aneurysms are diagnosed in 1-5% of pe...\n",
      "26    To investigate the effect of two fractionation...\n",
      "27    This study was conducted to determine the leve...\n",
      "28    Exercise-induced bronchoconstriction (EIB) is ...\n",
      "29    Gut microbiota and associated metabolites have...\n",
      "30    Drug Reaction with Eosinophilia and Systemic S...\n",
      "31    Cervical cancer screening is a cornerstone of ...\n",
      "32    Cardiac amyloidosis (CA) is characterized by d...\n",
      "33    This study aimed to analyze the demographic ch...\n",
      "34    Blastomycosis is an environmentally acquired f...\n",
      "35    Acute kidney injury (AKI) and chronic kidney d...\n",
      "36    Adverse childhood experiences (ACEs) have long...\n",
      "37    Reductions in all-cause otitis media (OM) foll...\n",
      "38    Cancer will affect more than one in three U.S....\n",
      "39    Renal cell carcinoma (RCC) is a highly aggress...\n",
      "40    We demonstrate the peculiarities of diagnosis ...\n",
      "41    A rare case of thyroid foreign body is present...\n",
      "42    To study the diagnostic value of fluorescent l...\n",
      "43    To create the prognostic scale based on some b...\n",
      "44    In this study, we aimed to identify bone miner...\n",
      "45    The early detection and accurate monitoring of...\n",
      "46    This study aimed to analyze the prevalence and...\n",
      "47    The glaucoma treatment paradigm is starting to...\n",
      "48    Hundreds of thousands of children are victims ...\n",
      "49    Despite a higher rate of breast cancer in sub-...\n",
      "Name: Abstract, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 242.83 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 3845.23 examples/s]\n",
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [05:40<00:00, 16.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 340.3298, 'train_samples_per_second': 0.264, 'train_steps_per_second': 0.062, 'train_loss': 1.8107521420433408, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:20<00:00, 10.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation: {'eval_loss': 2.204596757888794, 'eval_runtime': 20.5334, 'eval_samples_per_second': 0.487, 'eval_steps_per_second': 0.097, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=1024)\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "df = pd.read_csv('Question_4.csv') \n",
    "\n",
    "# Extract the abstracts\n",
    "abstracts = df['Abstract']\n",
    "abstracts = abstracts[:50]\n",
    "print(abstracts)\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": abstracts})\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "input_ids = tokenized_dataset['input_ids']\n",
    "labels = input_ids.clone()\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)\n",
    "\n",
    "train_size = 30\n",
    "val_size = 10\n",
    "test_size = 10\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    tokenized_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Update Trainer with train and validation sets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,    # Training set\n",
    "    eval_dataset=val_dataset,       # Validation set for tuning during training\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test set evaluation:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x00000156B9395690>\n",
      "Symptoms for breast cancer include pain, swelling, and itching. A rare but serious complication of lymphedema is cellulitis, which can result in disfigurement and functional impairment. To our knowledge, this is the first report of cellulitis secondary to lymphedema after breast-conserving treatment. The patient was a 66-year-old woman with breast carcinoma who developed cellulitis after receiving adjuvant radiotherapy and chemotherapy. She had swelling and pain in the left axilla, resulting in considerable functional loss. After the cellulitis resolved, she underwent axillary lymph node dissection with axillary sentinel lymph nodes biopsy. Histopathological examination revealed that the lymph vessels were dilated and tortuous, with focal fibrinoid\n",
      "Symptoms for breast cancer include pain, bleeding, and swelling. The diagnosis of breast lump is often delayed, which can lead to an increased risk of local recurrence and distant metastasis. Early detection and prompt treatment can reduce the risk for late-stage breast cancers. Therefore, it is important to identify patients at risk, as early detection can improve outcomes. This study aimed to examine the association between self-reported symptoms and the diagnosis and treatment of early-onset breast lumps. Data were collected as part of the Australian Breast Cancer Family Registry (ABCFR), a prospective cohort study of Australian women diagnosed with breast Cancer in 2013-2016\n",
      "Symptoms for breast cancer include pain, swelling and tenderness. The pain can be caused by local infiltration of the affected area or metastasis. Most women with breast carcinoma are treated with surgery and / or radiotherapy. However, a considerable proportion of patients may develop local or distant recurrence. This can cause significant complications including lymphedema and skin complications such as skin burns and ulcerations. In this paper, we present the case of a patient with a local recurrence of breast adenocarcinoma treated successfully with mastectomy and adjuvant chemotherapy and radiotherapy, with no signs of local relapse or metastases at six months after treatment. Although the treatment of recurrent breast cancers is complex, early diagnosis of complications\n",
      "Symptoms for breast cancer include pain, itching, and bleeding. The current standard of care for the treatment of breast cancers is surgery and / or radiotherapy, but the use of chemotherapy and targeted therapies has been increasing over the past few decades. A number of cytotoxic agents, including anthracyclines, taxanes, platinum compounds, vinca alkaloids, cyclophosphamide, doxorubicin, fluorouracil, irinotecan, capecitabine, bevacizumab, trastuzumab, erlotinib, gefitinib, imatinib, ramucirumab, pemetrexed, paclitaxel, gemcitabine, docetaxel, vorinostat, topotecan, ixabepilone, rituximab, panitumumab and bevacizumab combined with targeted therapy, have been\n",
      "Symptoms for breast cancer include pain, bleeding, swelling, and pruritus. The diagnosis of breast carcinoma is often delayed, which may lead to severe morbidity. Early recognition of the disease and treatment can reduce the likelihood of delayed diagnosis. This article presents a case of a patient with breast pain that was initially misdiagnosed as cellulitis. After extensive review of her medical records, the authors discovered that the patient's pain was related to breast inflammation. She was later diagnosed with invasive ductal carcinoma. Breast pain may be the presenting symptom of an underlying breast malignancy. To the best of their knowledge, this is the first case report of invasive breast ductal adenocarcinoma presenting as\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)\n",
    "\n",
    "prompt = \"Symptoms for breast cancer include\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  \n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_length=128,\n",
    "    num_return_sequences=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Delay in diagnosis and treatment of lung cance...\n",
      "1     As catabolic enzyme, CD73 dephosphorylates ade...\n",
      "2     Autophagy serves as a critical regulator of im...\n",
      "3     This study aims to compare the cost-effectiven...\n",
      "4     4D cone-beam computed tomography (CBCT) plays ...\n",
      "5     Utidelone is an ebomycin derivative chemothera...\n",
      "6     The enhancer of rudimentary homolog (ERH) is s...\n",
      "7     Liver cancer (LC) is among the deadliest cance...\n",
      "8     Lung cancer remains the most prevalent maligna...\n",
      "9     Improvements in cancer control have led to a d...\n",
      "10    Non-small cell lung cancer (NSCLC) is the lead...\n",
      "11    Multiple microRNAs encapsulated in extracellul...\n",
      "12    Cisplatin resistance is common in nonâ€‘small ce...\n",
      "13    Cancer will affect more than one in three U.S....\n",
      "14    Thyroid cancer is the most prevalent form of e...\n",
      "15    With significant advancements in the study of ...\n",
      "16    Dysregulation of lung tissue collagen level pl...\n",
      "17    Primary gastric small cell carcinoma (GSCC) is...\n",
      "18    The primary goal of this research is to develo...\n",
      "19    Lung cancer is the most common type of cancer,...\n",
      "20    Several tyrosine kinase receptors inhibitors (...\n",
      "21    India is a vast and diverse country with exist...\n",
      "22    Radiotherapy (RT) is one of the primary treatm...\n",
      "23    Lung Cancer remains one of the leading causes ...\n",
      "24    Lung squamous cell carcinoma (LUSC) is the sec...\n",
      "25    Primary hepatoid adenocarcinoma of the lung (H...\n",
      "26    This study was designed to investigate the cli...\n",
      "27    Human epidermal growth factor receptor 2 (HER2...\n",
      "28    Lung cancer remains a leading cause of cancer-...\n",
      "29    To evaluate the prognostic impact of neutrophi...\n",
      "30    Volumetric modulated arc therapy (VMAT) is a n...\n",
      "31    To assess the molecular profile of follicular ...\n",
      "32    Human non-small cell lung cancer (NSCLC)is a v...\n",
      "33    The abnormality of the immune system caused by...\n",
      "34    This study aimed to conduct a systematic revie...\n",
      "35    Breast cancer with human epidermal growth fact...\n",
      "36    To explore the application value of a multimod...\n",
      "37    Dual bronchodilator therapy, consisting of a l...\n",
      "38    Low-fat dietary (LFD) pattern refers to a diet...\n",
      "39    In small cell lung cancer (SCLC), the patholog...\n",
      "40    The release of extracellular vesicles (EVs) in...\n",
      "41    The single-inhaler triple combination of beclo...\n",
      "42    Non-small cell lung cancer (NSCLC) is a highly...\n",
      "43    Initially, identified as a Hodgkin lymphoma ma...\n",
      "44    High heterogeneity in clinical benefit charact...\n",
      "45    A tendon's ordered extracellular matrix (ECM) ...\n",
      "46    This study explores the relationship between m...\n",
      "47    To study programmed death ligand 1 (PD-L1) exp...\n",
      "48    The plasma concentration of the macroautophagy...\n",
      "49    Non-small cell lung cancer (NSCLC) remains a l...\n",
      "Name: Abstract, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 284.53 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 3767.52 examples/s]\n",
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [09:33<00:00, 27.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 573.8683, 'train_samples_per_second': 0.157, 'train_steps_per_second': 0.037, 'train_loss': 2.098057156517392, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation: {'eval_loss': 1.9998762607574463, 'eval_runtime': 17.6431, 'eval_samples_per_second': 0.567, 'eval_steps_per_second': 0.113, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=1024)\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "df = pd.read_csv('Question_5.csv') \n",
    "\n",
    "# Extract the abstracts\n",
    "abstracts = df['Abstract']\n",
    "abstracts = abstracts[:50]\n",
    "print(abstracts)\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": abstracts})\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "input_ids = tokenized_dataset['input_ids']\n",
    "labels = input_ids.clone()\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)\n",
    "\n",
    "train_size = 30\n",
    "val_size = 10\n",
    "test_size = 10\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    tokenized_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Update Trainer with train and validation sets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,    # Training set\n",
    "    eval_dataset=val_dataset,       # Validation set for tuning during training\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test set evaluation:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x00000157B4A83790>\n",
      "Lung cancer is the leading cause of cancer-related deaths worldwide. A substantial number of lung cancer patients are diagnosed at an advanced stage of the disease, resulting in a poor prognosis. The standard of care for advanced non-small cell lung cancers (NSCLC) is platinum-based chemotherapy, with targeted therapies and immunotherapy being investigated for the treatment of advanced NSCLC. This review summarizes the state of knowledge regarding the use of targeted therapy, immunotherapy and combination therapies in advanced lung adenocarcinoma. We provide a comprehensive review of clinical trials investigating targeted agents, immunotherapies and combined treatments for lung adenocarcinomas. Expert opinion: The advent of new targeted drugs has changed the landscape\n",
      "Lung cancer is a leading cause of cancer-related deaths worldwide, with the majority of patients diagnosed in late stages. The diagnosis of lung cancer at an earlier stage can improve patient survival and reduce treatment-associated toxicities. Lung Cancer is the leading type of non-small cell lung carcinoma (NSCLC) and accounts for approximately 85% of all lung cancers diagnosed. In this study, we used a combination of proteomics and metabolomics to investigate the molecular changes induced by chemotherapy in the blood of mice bearing Lewis lung carcinomas (LLC). Male C57BL / 6 mice were inoculated with LLC cells, which were treated with either cisplatin or cisplatin plus docetaxel. Serum\n",
      "Lung cancer is the leading cause of cancer related deaths worldwide. The majority of lung cancer cases are diagnosed at an advanced stage with a poor prognosis. Lung adenocarcinoma (LUAD) is one of the most common types of non-small cell lung cancers. It is characterized by a unique genetic landscape, which can be targeted by targeted therapies. In recent years, several targeted therapeutics have been developed for the treatment of LUADs. However, the resistance of tumors to these therapies is still a major challenge. Here, we review the current targeted treatments for LUAD.We summarize the molecular mechanisms of resistance to targeted therapeutic agents. We also discuss the\n",
      "Lung cancer is the leading cause of cancer-related deaths worldwide. The lung cancer has unique characteristics that lead to a diverse range of clinical manifestations. This study aimed to characterize the clinical presentation and treatment outcomes in patients with lung adenocarcinoma harboring epidermal growth factor receptor (EGFR) mutations, compared with patients without EGFR mutations. We retrospectively analyzed patients diagnosed with primary lung adenocarcinomas between January 2014 and January 2019 in the Department of Thoracic Surgery, Cancer Hospital of Chinese Academy of Medical Sciences. Data on demographic characteristics, clinical symptoms, laboratory tests, treatment, and follow-up were collected. A total of 718 patients were included in this study. Of these\n",
      "Lung cancer is the leading cause of cancer-related deaths worldwide. The incidence of lung cancer has been increasing in China. Lung adenocarcinoma is an invasive type of non-small cell lung carcinoma, with a more aggressive and invasive growth pattern. In this study, we aimed to identify the potential targets of natural compounds in lung adenocarcinoma and explore the effects of the combination of two natural agents (curcumin and baicalin) on cell proliferation and apoptosis in human lung A549 cell lines. We used a combination index (CI) method to analyze the growth inhibition of A549 cells in the presence of different concentrations of curcumin, baicalein, or the two compounds\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)\n",
    "\n",
    "prompt = \"Lung cancer is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  \n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_length=128,\n",
    "    num_return_sequences=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Paclitaxel-induced peripheral neuropathy (PN) ...\n",
      "1     Breast cancer is the leading cause of cancer-r...\n",
      "2     The study objective was to determine the effec...\n",
      "3     One of the main health issues in the modern wo...\n",
      "4     There are currently no molecular tests to iden...\n",
      "5     Utidelone is an ebomycin derivative chemothera...\n",
      "6     Neoadjuvant endocrine therapy presents an impo...\n",
      "7     Social support has been linked to increased us...\n",
      "8     Sialyltransferases are enzymes that play a cru...\n",
      "9     Eribulin is an inhibitor of microtubule dynami...\n",
      "10    Benzofuropyridines (BFP) are polycyclic compou...\n",
      "11    This study was conducted to determine the leve...\n",
      "12    Improvements in cancer control have led to a d...\n",
      "13    Gut microbiota and associated metabolites have...\n",
      "14    Breast cancer (BC) remains a prevalent and cha...\n",
      "15    Differential RNA expression is becoming increa...\n",
      "16    Cancer will affect more than one in three U.S....\n",
      "17    Saltâ€‘induced kinase 1 (SIK1) is a serine/threo...\n",
      "18    To study the diagnostic value of fluorescent l...\n",
      "19    Breast Cancer stands on the second position in...\n",
      "20    With significant advancements in the study of ...\n",
      "21    In the present study, a novel series of sulfam...\n",
      "22    Despite a higher rate of breast cancer in sub-...\n",
      "23    We present the use of stimulated Brillouin sca...\n",
      "24    The relationship between the albumin-to-globul...\n",
      "25    Proteolysis-targeting chimeras (PROTACs) have ...\n",
      "26    India is a vast and diverse country with exist...\n",
      "27    Mindfulness-based stress reduction (MBSR) has ...\n",
      "28    The anti-angiogenic agent anlotinib offers a n...\n",
      "29    Randomised evidence supports the use of partia...\n",
      "30    Since 2020, breast cancer has remained the mos...\n",
      "31    In addition to blood test data, inflammation-b...\n",
      "32    Platypnea-orthodeoxia syndrome (POS) is an unc...\n",
      "33    Among the 7.8 million women with breast cancer...\n",
      "34    Breast cancer poses a significant global healt...\n",
      "35    Delays in initiating cancer treatment time to ...\n",
      "36    The Government of India (GOI) has launched a n...\n",
      "37    Vasculitides are a set of pathologies that can...\n",
      "38    Breast cancer is a common and complex disease,...\n",
      "39    [This corrects the article DOI: 10.3389/fnins....\n",
      "40    Breast cancer is the most common malignancy am...\n",
      "41    Neural cell adhesion molecule (NCAM), a common...\n",
      "42    Radiotherapy, a common cancer treatment, leads...\n",
      "43    The CRISPR-Cas systems are adopted as powerful...\n",
      "44    With the advancement of computer technology an...\n",
      "45    We examined neighborhood characteristics conce...\n",
      "46    Metastasis to the oral soft tissues and jaw is...\n",
      "47    Inflammatory breast carcinoma (IBC) is an aggr...\n",
      "48    We report the synthesis, characterization, ant...\n",
      "49    The integrated behaviour of multiple senescent...\n",
      "Name: Abstract, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 261.61 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 2324.85 examples/s]\n",
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [04:40<00:00, 13.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 280.142, 'train_samples_per_second': 0.321, 'train_steps_per_second': 0.075, 'train_loss': 2.1254348754882812, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation: {'eval_loss': 2.038769245147705, 'eval_runtime': 3.1513, 'eval_samples_per_second': 3.173, 'eval_steps_per_second': 0.635, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=1024)\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "df = pd.read_csv('Question_6.csv') \n",
    "\n",
    "# Extract the abstracts\n",
    "abstracts = df['Abstract']\n",
    "abstracts = abstracts[:50]\n",
    "print(abstracts)\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": abstracts})\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "input_ids = tokenized_dataset['input_ids']\n",
    "labels = input_ids.clone()\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)\n",
    "\n",
    "train_size = 30\n",
    "val_size = 10\n",
    "test_size = 10\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    tokenized_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Update Trainer with train and validation sets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,    # Training set\n",
    "    eval_dataset=val_dataset,       # Validation set for tuning during training\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test set evaluation:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x0000015718E2BF10>\n",
      "Breast cancer is the most common cancer affecting women worldwide. The incidence of this malignancy is increasing worldwide, with the highest incidence in developing countries. Despite significant advances in the diagnosis and treatment of breast cancer, the disease remains a major cause of mortality in women. New diagnostic and therapeutic methods are needed to improve the treatment outcomes for this cancer. In this review, we summarize the current advances and future prospects of non-invasive, real-time detection of circulating tumor cells (CTCs). We also discuss the clinical implications of CTCs in cancer treatment, including the use of cell-free DNA (cfDNA) as a biomarker for monitoring treatment response,\n",
      "Breast cancer is the most common malignancy among women and the second leading cause of cancer-related death in women worldwide. The present study aimed to investigate the association between serum vitamin D levels and breast cancer among patients. A total of 140 patients with breast carcinoma and 140 healthy controls were included in the present case-control study. Vitamin D status was assessed using an enzyme-linked immunosorbent assay (ELISA). No significant differences were observed between the two groups in terms of age, body mass index (BMI), tumor size, or the number of lymph nodes. However, the vitamin A level was significantly higher in patients compared with controls (P\n",
      "Breast cancer is a leading cause of cancer-related deaths in the world, and the incidence is increasing. A major cause for this increase is the poor understanding of the molecular mechanisms underlying breast cancer progression. The development of targeted therapies for breast cancers has resulted in increased patient survival; however, these therapies have failed to significantly improve the survival of patients with triple-negative breast carcinomas. Recent research has identified a number of important driver genes involved in breast carcinogenesis, which may serve as potential biomarkers for predicting the response to targeted therapy. However, the clinical significance of these biomarkers is unclear. To investigate the prognostic value of several driver gene mutations in patients\n",
      "Breast cancer is a common cancer in women worldwide. The 5-year survival rate is 80%. This study aimed to determine the association between breast cancer risk perception and the risk of breast invasive cancer. A total of 2022 women aged 18 years and above were enrolled in this study. Data were collected through a self-administered questionnaire including sociodemographic variables, risk perceptions, and breast screening. Breast Cancer Risk Perception Scale-A (BCRS-As) was used to assess the participants' risk level. Descriptive statistics were used for data analysis, including mean, standard deviation, median, interquartile range, frequency, percentage, percentages, means,\n",
      "Breast cancer is a complex disease that is characterized by the presence of heterogeneous tumor subtypes. A growing body of evidence demonstrates that epigenetic processes, including DNA methylation and histone modifications, play a role in breast cancer progression. DNA hypermethylation is associated with gene silencing, and epigenetic alterations are implicated in several human cancers, in particular in the regulation of the expression of genes encoding proteins involved in tumor progression and metastasis. Aberrant histone methylation is an epigenetic mechanism that regulates the gene expression, leading to an increase in gene transcription. Histone modifications are reversible, with histone methyltransferases (HMT) and demethylases (HDMs) being key regulators of these processes.\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)\n",
    "\n",
    "prompt = \"Breast cancer is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  \n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_length=128,\n",
    "    num_return_sequences=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Pancreatic adenosquamous carcinoma (PASC) is a...\n",
      "1     Sonodynamic therapy (SDT) is a promising strat...\n",
      "2     Pancreatic ductal adenocarcinoma (PDAC), the m...\n",
      "3     Pancreatoduodenectomy (PD) is a highly complex...\n",
      "4     V-domain Imuunoglobulin suppressor of T-cell a...\n",
      "5     Anaplastic thyroid cancer (ATC) is one of the ...\n",
      "6     Several tyrosine kinase receptors inhibitors (...\n",
      "7     Circulating tumor cells (CTCs) are tumor cells...\n",
      "8     Indoleamine 2,3-dioxygenase 1 (IDO1) plays a k...\n",
      "9     Tumor microenvironment (TME) is essential for ...\n",
      "10    Thromboembolic events (TEEs) are frequent amon...\n",
      "11    The aggressiveness of pancreatic ductal adenoc...\n",
      "12    Disconnected pancreatic duct syndrome (DPDS) i...\n",
      "13    Pancreatic ductal adenocarcinoma (PDAC) poses ...\n",
      "14    The purpose of this study was to translate the...\n",
      "15    Immunotherapy has made significant strides in ...\n",
      "16    Type 1 diabetes (T1D) is characterized by an a...\n",
      "17    Ubiquitination is one of the important modific...\n",
      "18    This meta-analysis aimed to compare the effica...\n",
      "19    Robotic surgery has demonstrated outcomes comp...\n",
      "20    Risk factors for pancreatic ductal adenocarcin...\n",
      "21    CD200, a negative regulator of T cells as well...\n",
      "22    To investigate the practices of clinicians pre...\n",
      "23    Surgical resection is the standard recommended...\n",
      "24    Enteroviruses, which infect via the gut, have ...\n",
      "25    Colorectal cancer (CRC), ranked as the globe's...\n",
      "26    L-type amino acid transporter 1 (LAT1) is a me...\n",
      "27    Pancreatic cancer, characterized by a dismal p...\n",
      "28    The peptidyl-prolyl cis/trans isomerase NIMA-i...\n",
      "29    The incidence of GI cancers is increasing in s...\n",
      "30    This study aims to clarify the benefit of adju...\n",
      "31    Porcine cancer models offer a valuable platfor...\n",
      "32    Despite the presence of various guidelines, di...\n",
      "33    Data on the effects of glucagon-like peptide-1...\n",
      "34    Associations of body mass index (BMI) with sur...\n",
      "35    Combined pancreatic and vascular resections ar...\n",
      "36    Chimeric antigen receptor (CAR) T-cell therapy...\n",
      "37    KRAS mutations occur in ~40-50% of mCRC and ar...\n",
      "38    Pancreatic cancer is an aggressive and metasta...\n",
      "39    Pancreatic cancer is an aggressive cancer with...\n",
      "40    Pancreatic ductal adenocarcinoma (PDAC) patien...\n",
      "41    Immunogenic cell death (ICD) can be exploited ...\n",
      "42    The outcome of clinical trials evaluating drug...\n",
      "43    Several studies have evaluated the impact of s...\n",
      "44    Pancreatic cancer (PC) is a highly aggressive ...\n",
      "45    Pancreatic cancer is usually detected through ...\n",
      "46    Homologous recombination (HR) is an important ...\n",
      "47    Organs in the abdominal cavity are covered by ...\n",
      "48    Immunotherapy is an emerging strategy in cance...\n",
      "49    The discovery of embryonic stem cell (ESC) med...\n",
      "Name: Abstract, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 228.92 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 3124.39 examples/s]\n",
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [07:49<00:00, 67.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 469.9399, 'train_samples_per_second': 0.064, 'train_steps_per_second': 0.015, 'train_loss': 2.2471866607666016, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation: {'eval_loss': 1.9083530902862549, 'eval_runtime': 207.2702, 'eval_samples_per_second': 0.048, 'eval_steps_per_second': 0.01, 'epoch': 0.9333333333333333}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=1024)\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "df = pd.read_csv('Question_7.csv') \n",
    "\n",
    "# Extract the abstracts\n",
    "abstracts = df['Abstract']\n",
    "abstracts = abstracts[:50]\n",
    "print(abstracts)\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": abstracts})\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "input_ids = tokenized_dataset['input_ids']\n",
    "labels = input_ids.clone()\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)\n",
    "\n",
    "train_size = 30\n",
    "val_size = 10\n",
    "test_size = 10\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    tokenized_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Update Trainer with train and validation sets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,    # Training set\n",
    "    eval_dataset=val_dataset,       # Validation set for tuning during training\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test set evaluation:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x0000013872AD5630>\n",
      "Pancreatic cancer is a leading cause of cancer-related death worldwide. A pancreatic cancer stem cell (CSC) model has been developed, in which the pancreatic CSC subpopulation is enriched by enriching cells in a serum-free, chemically defined medium containing epidermal growth factor, hepatocyte growth factors, and insulin-like growth Factor-2. The pancreatic CSCs are characterized by a high level of aldehyde dehydrogenase (ALDH) activity and cancer cell self-renewal. Here, we investigated the effect of resveratrol (RSV), a polyphenol compound, on the proliferation and viability of pancreatic ductal adenocarcinoma (PDAC) cells, as well as on pancreatic and CSC characteristics.\n",
      "Pancreatic cancer is a highly invasive and fatal cancer. A review of clinical studies using the most frequently used therapeutic approaches: chemotherapy, targeted therapy and immunotherapy. The aim of this review is to discuss the current evidence regarding the treatment of pancreatic cancer, focusing on chemotherapy and targeted therapies. Currently, in the majority of cases, treatment is based on the use of a combination of chemotherapy with targeted agents or immunotherapy; however, there is no evidence supporting the routine use or the best regimen for the management of patients with pancreatic adenocarcinoma. New therapeutic strategies are being developed for patients who have already progressed on these treatments. This is particularly important given the poor prognosis\n",
      "Pancreatic cancer is a highly aggressive malignancy that has a poor prognosis.\n",
      "Pancreatic cancer (PC) is the fourth leading cause of cancer-related deaths worldwide and is usually diagnosed at advanced stages with a poor prognosis.\n",
      "Pancreatic cancer (PC) is one of the most aggressive malignancies and is associated with high mortality and a dismal prognosis. The development of novel therapies is urgently needed. Here, we demonstrate that the microRNA-155 (miR-155m) expression level was significantly elevated in PC tissues and cell lines. Overexpression of miR â€‘ 153m in pancreatic cancer cells promoted cell growth, migration and invasion, whereas its inhibition inhibited the cell proliferation, invasion and migration of PC cells. Furthermore, the mechanistic studies revealed that miR -153b directly targeted the 3 'untranslated region of MLL3, a well â€‘ known tumor suppressor gene. In\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)\n",
    "\n",
    "prompt = \"Pancreatic cancer is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  \n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_length=128,\n",
    "    num_return_sequences=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=1024)\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "df = pd.read_csv('Question_8.csv') \n",
    "\n",
    "# Extract the abstracts\n",
    "abstracts = df['Abstract']\n",
    "abstracts = abstracts[:50]\n",
    "print(abstracts)\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": abstracts})\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "input_ids = tokenized_dataset['input_ids']\n",
    "labels = input_ids.clone()\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)\n",
    "\n",
    "train_size = 30\n",
    "val_size = 10\n",
    "test_size = 10\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    tokenized_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Update Trainer with train and validation sets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,    # Training set\n",
    "    eval_dataset=val_dataset,       # Validation set for tuning during training\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test set evaluation:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dataset)\n",
    "\n",
    "prompt = \"Symptoms for pancreatic cancer include\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  \n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_length=128,\n",
    "    num_return_sequences=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     To investigate the practices of clinicians pre...\n",
      "1     Postoperative pancreatic fistula (POPF) contin...\n",
      "2     STK11 germline pathogenic variants are typical...\n",
      "3     The overall treatment response among patients ...\n",
      "4     Post-transplant HCC recurrence significantly i...\n",
      "5     Patients with synchronous pancreatic ductal ad...\n",
      "6     This research aimed to assess the value of rad...\n",
      "7     Pancreatic Ductal Adenocarcinoma (PDAC) primar...\n",
      "8     Muscular dystrophies and myotonic disorders ar...\n",
      "9     Tumor-infiltrating lymphocytes (TILs) are sign...\n",
      "10    Although atezolizumab plus bevacizumab (Atezo/...\n",
      "11    Cutaneous malignant melanoma is one of the mos...\n",
      "12    Pancreatic cancer, the 12th-most common cancer...\n",
      "13    Cancer immunotherapy using immune checkpoint i...\n",
      "14    Given the extensive role of lipids in cancer d...\n",
      "15    Pancreatic cancer is a prevalent malignant tum...\n",
      "16    Conducting\"health economic evaluation\"is one o...\n",
      "17    The prognosis of pancreatic cancer (PDAC) afte...\n",
      "18    The incidence of pancreatic cystic neoplasms i...\n",
      "19    Germline breast cancer susceptibility gene (gB...\n",
      "20    Single large hepatocellular carcinoma >5cm (SL...\n",
      "21    Lenvatinib (dosing for patients who weigh â‰¥60 ...\n",
      "22    To investigate in patients treated for a resec...\n",
      "23    Biomarkers for prediction of outcome in patien...\n",
      "24    This editorial discusses the article \"Analysis...\n",
      "25    Colon adenocarcinoma (COAD) is a prevalent mal...\n",
      "26    Hepatocellular carcinoma (HCC) is one of the m...\n",
      "27    Progressive functional decline is a key elemen...\n",
      "28    The prognosis of Klatskin tumors is poor, and ...\n",
      "29    Primary pancreatic lymphomas (PPLs) are a subg...\n",
      "30    Significant progress has been made in the mana...\n",
      "31    Neoadjuvant treatment (NAT) is standard for bo...\n",
      "32    Surgical resection (SR) is the main treatment ...\n",
      "33    Neuroendocrine neoplasms are a diverse group o...\n",
      "34    For reliable DCE MRI parameter estimation, k-s...\n",
      "35    The purpose of the study is to construct meani...\n",
      "36    Pancreatic ductal adenocarcinoma is an aggress...\n",
      "37    Both the incidence and prevalence of well-diff...\n",
      "38    Tumour morphology (tumour burden score (TBS)) ...\n",
      "39    We aimed to establish global benchmark outcome...\n",
      "40    Parenteral nutrition (PN) can be an effective ...\n",
      "41    Apart from cytotoxicity, inhibitors of the COX...\n",
      "42    To evaluate the quality of guidelines on the p...\n",
      "43    This is a case of a 67-year-old woman diagnose...\n",
      "44    Posthepatectomy liver failure (PHLF), complica...\n",
      "45    Developing clinically meaningful nanomedicines...\n",
      "46    This study was designed to explore and validat...\n",
      "47    To assess the utility of tumor-intrinsic and c...\n",
      "48    Gastroenteropancreatic neuroendocrine tumors (...\n",
      "49    Electron Paramagnetic Resonance Imaging (EPRI)...\n",
      "Name: Abstract, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 225.81 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 2082.90 examples/s]\n",
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 67\u001b[0m\n\u001b[0;32m     58\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     59\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     60\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m     64\u001b[0m )\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m metrics \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(test_dataset)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest set evaluation:\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2394\u001b[0m ):\n\u001b[0;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\trainer.py:3518\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3516\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3518\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\accelerate\\accelerator.py:2196\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2196\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=1024)\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "df = pd.read_csv('Question_9.csv') \n",
    "\n",
    "# Extract the abstracts\n",
    "abstracts = df['Abstract']\n",
    "abstracts = abstracts[:50]\n",
    "print(abstracts)\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": abstracts})\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "input_ids = tokenized_dataset['input_ids']\n",
    "labels = input_ids.clone()\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)\n",
    "\n",
    "train_size = 30\n",
    "val_size = 10\n",
    "test_size = 10\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    tokenized_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Update Trainer with train and validation sets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,    # Training set\n",
    "    eval_dataset=val_dataset,       # Validation set for tuning during training\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test set evaluation:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dataset)\n",
    "\n",
    "prompt = \"Treatment for pancreatic cancer include\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  \n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_length=128,\n",
    "    num_return_sequences=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BioGptForCausalLM(\n",
       "  (biogpt): BioGptModel(\n",
       "    (embed_tokens): BioGptScaledWordEmbedding(42384, 1024, padding_idx=1)\n",
       "    (embed_positions): BioGptLearnedPositionalEmbedding(1026, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x BioGptDecoderLayer(\n",
       "        (self_attn): BioGptSdpaAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (activation_fn): GELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (output_projection): Linear(in_features=1024, out_features=42384, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "# from datasets import Dataset\n",
    "# import torch\n",
    "\n",
    "# tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "# model = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")\n",
    "# model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 231.73 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 2726.47 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 221.59 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 2999.36 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 200.66 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 3332.43 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 197.82 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 2788.58 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 232.32 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 3332.69 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 216.81 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 3526.80 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 204.75 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 3332.60 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 186.80 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 2726.94 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 195.31 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 3332.34 examples/s]\n",
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                      \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [04:30<00:00, 27.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 270.2112, 'train_samples_per_second': 0.148, 'train_steps_per_second': 0.037, 'train_loss': 1.9663129806518556, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:23<00:00, 11.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation: {'eval_loss': 2.1346869468688965, 'eval_runtime': 23.3001, 'eval_samples_per_second': 0.429, 'eval_steps_per_second': 0.086, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "# from datasets import Dataset, concatenate_datasets\n",
    "# import pandas as pd\n",
    "# from datasets import Dataset\n",
    "\n",
    "# tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "# q1 = pd.read_csv('Question_1.csv') \n",
    "# q2 = pd.read_csv('Question_2.csv')\n",
    "# q3 = pd.read_csv('Question_3.csv')\n",
    "# q4 = pd.read_csv('Question_4.csv')\n",
    "# q5 = pd.read_csv('Question_5.csv')\n",
    "# q6 = pd.read_csv('Question_6.csv')\n",
    "# q7 = pd.read_csv('Question_7.csv')\n",
    "# q8 = pd.read_csv('Question_8.csv')\n",
    "# q9 = pd.read_csv('Question_9.csv')\n",
    "\n",
    "# a1 = q1['Abstract'][:30]\n",
    "# a2 = q2['Abstract'][:30]\n",
    "# a3 = q3['Abstract'][:30]\n",
    "# a4 = q4['Abstract'][:30]\n",
    "# a5 = q5['Abstract'][:30]\n",
    "# a6 = q6['Abstract'][:30]\n",
    "# a7 = q7['Abstract'][:30]\n",
    "# a8 = q8['Abstract'][:30]\n",
    "# a9 = q9['Abstract'][:30]\n",
    "\n",
    "# def preprocess_function(examples):\n",
    "#     return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=1024)\n",
    "\n",
    "# def get_abstracts(abstracts):\n",
    "#     dataset = Dataset.from_dict({\"text\": abstracts})\n",
    "#     tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "#     tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "#     tokenized_dataset.set_format(\"torch\")\n",
    "#     tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "#     return tokenized_dataset\n",
    "\n",
    "# tokenized_dataset1 = get_abstracts(a1)\n",
    "# tokenized_dataset2 = get_abstracts(a2)\n",
    "# tokenized_dataset3 = get_abstracts(a3)\n",
    "# tokenized_dataset4 = get_abstracts(a4)\n",
    "# tokenized_dataset5 = get_abstracts(a5)\n",
    "# tokenized_dataset6 = get_abstracts(a6)\n",
    "# tokenized_dataset7 = get_abstracts(a7)\n",
    "# tokenized_dataset8 = get_abstracts(a8)\n",
    "# tokenized_dataset9 = get_abstracts(a9)\n",
    "\n",
    "# def subset_to_dataset(subset):\n",
    "#     return Dataset.from_dict(subset.dataset[subset.indices])\n",
    "\n",
    "# def get_datasets(tokenized_dataset):\n",
    "#     train_size = 20\n",
    "#     val_size = 5\n",
    "#     test_size = 5\n",
    "\n",
    "#     train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "#         tokenized_dataset, [train_size, val_size, test_size]\n",
    "#     )\n",
    "#     return subset_to_dataset(train_dataset), subset_to_dataset(val_dataset), subset_to_dataset(test_dataset)\n",
    "\n",
    "# a1_train_dataset, a1_val_dataset1, a1_test_dataset1 = get_datasets(tokenized_dataset1)\n",
    "# a2_train_dataset, a2_val_dataset2, a2_test_dataset2 = get_datasets(tokenized_dataset2)\n",
    "# a3_train_dataset, a3_val_dataset3, a3_test_dataset3 = get_datasets(tokenized_dataset3)\n",
    "# a4_train_dataset, a4_val_dataset4, a4_test_dataset4 = get_datasets(tokenized_dataset4)\n",
    "# a5_train_dataset, a5_val_dataset5, a5_test_dataset5 = get_datasets(tokenized_dataset5)\n",
    "# a6_train_dataset, a6_val_dataset6, a6_test_dataset6 = get_datasets(tokenized_dataset6)\n",
    "# a7_train_dataset, a7_val_dataset7, a7_test_dataset7 = get_datasets(tokenized_dataset7)\n",
    "# a8_train_dataset, a8_val_dataset8, a8_test_dataset8 = get_datasets(tokenized_dataset8)\n",
    "# a9_train_dataset, a9_val_dataset9, a9_test_dataset9 = get_datasets(tokenized_dataset9)\n",
    "\n",
    "\n",
    "# final_test_dataset = concatenate_datasets([a1_test_dataset1,  a7_test_dataset7])\n",
    "# final_train_dataset = concatenate_datasets([a1_train_dataset, a7_train_dataset])\n",
    "# final_val_dataset = concatenate_datasets([a1_val_dataset1, a7_val_dataset7])\n",
    "\n",
    "# # print(final_train_dataset)\n",
    "\n",
    "# input_ids = final_train_dataset['input_ids']\n",
    "# labels = input_ids\n",
    "\n",
    "# from transformers import DataCollatorForLanguageModeling\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# # Define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     learning_rate=2e-5,\n",
    "#     lr_scheduler_type=\"linear\",\n",
    "#     seed=42,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     num_train_epochs=1,\n",
    "#     weight_decay=0.01,\n",
    "#     gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "#     metric_for_best_model=\"loss\",\n",
    "#     greater_is_better=False,\n",
    "#     load_best_model_at_end=True,\n",
    "# )\n",
    "\n",
    "\n",
    "# # Update Trainer with train and validation sets\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=final_train_dataset,    # Training set\n",
    "#     eval_dataset=final_val_dataset,       # Validation set for tuning during training\n",
    "#     data_collator=data_collator,\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train()\n",
    "# metrics = trainer.evaluate(final_test_dataset)\n",
    "# print(\"Test set evaluation:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_text(prompt):\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model.to(device)  \n",
    "#     inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "#     outputs = model.generate(\n",
    "#         input_ids=inputs[\"input_ids\"],\n",
    "#         max_length=128,\n",
    "#         num_return_sequences=5, \n",
    "#         no_repeat_ngram_size=2, \n",
    "#         do_sample=True,\n",
    "#         top_k=50,\n",
    "#         top_p=0.95,\n",
    "#         temperature=0.7,\n",
    "#     )\n",
    "\n",
    "#     print('Prompt:', prompt)\n",
    "#     for i in range(5):\n",
    "#         generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "#         print(generated_text)\n",
    "\n",
    "\n",
    "# generate_text(\"Treatment for lung cancer includes\")\n",
    "# generate_text(\"Treatment for breast cancer includes\")\n",
    "# generate_text(\"Symptoms for lung cancer include\")\n",
    "# generate_text(\"Symptoms for breast cancer include\")\n",
    "# generate_text(\"Lung cancer is\")\n",
    "# generate_text(\"Breast cancer is\")\n",
    "# generate_text(\"Pancreatic cancer is\")\n",
    "# generate_text(\"Symptoms for pancreatic cancer include\")\n",
    "# generate_text(\"Treatment for pancreatic cancer include\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write the final dataset to a csv file\n",
    "# final_train_dataset.to_csv('final_train_dataset.csv')\n",
    "# final_val_dataset.to_csv('final_val_dataset.csv')\n",
    "# final_test_dataset.to_csv('final_test_dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
