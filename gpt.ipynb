{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\almond\\anaconda3\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: requests in c:\\users\\almond\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\almond\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\almond\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\almond\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\almond\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\almond\\anaconda3\\lib\\site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\almond\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\almond\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\almond\\anaconda3\\lib\\site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\almond\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\almond\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\almond\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\almond\\anaconda3\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\almond\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\almond\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\almond\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (1.26.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sacremoses in c:\\users\\almond\\anaconda3\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: click in c:\\users\\almond\\anaconda3\\lib\\site-packages (from sacremoses) (8.0.4)\n",
      "Requirement already satisfied: regex in c:\\users\\almond\\anaconda3\\lib\\site-packages (from sacremoses) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\almond\\anaconda3\\lib\\site-packages (from sacremoses) (4.66.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\almond\\anaconda3\\lib\\site-packages (from sacremoses) (1.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\almond\\anaconda3\\lib\\site-packages (from click->sacremoses) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in c:\\users\\almond\\anaconda3\\lib\\site-packages (1.12.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\almond\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets in c:\\users\\almond\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\almond\\anaconda3\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\almond\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\almond\\anaconda3\\lib\\site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\almond\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\almond\\anaconda3\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\almond\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.6.1,>=2023.1.0 in c:\\users\\almond\\anaconda3\\lib\\site-packages (from datasets) (2024.6.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\almond\\anaconda3\\lib\\site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\almond\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\almond\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\almond\\anaconda3\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in c:\\users\\almond\\anaconda3\\lib\\site-packages (from datasets) (0.25.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\almond\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\almond\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\almond\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\almond\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\almond\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\almond\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\almond\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\almond\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\almond\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\almond\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.22.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\almond\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.32.2->datasets) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\almond\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.32.2->datasets) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\almond\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\almond\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\almond\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\almond\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\almond\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install sacremoses\n",
    "%pip install torch\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BioGptForCausalLM(\n",
       "  (biogpt): BioGptModel(\n",
       "    (embed_tokens): BioGptScaledWordEmbedding(42384, 1024, padding_idx=1)\n",
       "    (embed_positions): BioGptLearnedPositionalEmbedding(1026, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x BioGptDecoderLayer(\n",
       "        (self_attn): BioGptSdpaAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (activation_fn): GELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (output_projection): Linear(in_features=1024, out_features=42384, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "model = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Osimertinib is used as the first-line treatmen...\n",
      "1     We wanted to present a rare case of metastatic...\n",
      "2     Chemotherapy in an integral part of cancer tre...\n",
      "3     DNA methylation plays a regulatory role in the...\n",
      "4     Lung cancer is the leading cause of cancer-rel...\n",
      "5     Diffuse large B-cell lymphoma is the most comm...\n",
      "6     To evaluate the efficacy and safety of program...\n",
      "7     Breast cancer (BC) is the most frequently occu...\n",
      "8     Breast cancer has the potential to metastasize...\n",
      "9     Bronchiolitis obliterans syndrome (BOS) occurr...\n",
      "10    For drugs with a narrow therapeutic window, th...\n",
      "11    The IROC head and neck phantom is used to cred...\n",
      "12    To compare the efficacy of first-line regimens...\n",
      "13    This systematic review and network meta-analys...\n",
      "14    Accurate staging improves lung cancer survival...\n",
      "15    This research aimed to systematically uncover ...\n",
      "16    Lymph node metastasis (LNM) plays a crucial ro...\n",
      "17    Lung cancer is a leading cause of cancer-relat...\n",
      "18    Durvalumab is an IgG1 monoclonal antibody that...\n",
      "19    The advent of immune checkpoint inhibitors (IC...\n",
      "20    Small cell lung cancer (SCLC) is a highly inva...\n",
      "21    With the advent of targeted therapies, the sur...\n",
      "22    A dosimetric evaluation is still lacking in te...\n",
      "23    Nutritional status assessment has been deemed ...\n",
      "24    Ranked high in worldwide growing health issues...\n",
      "25    The optimal number of lymph nodes to be dissec...\n",
      "26    Carinal reconstruction remains a technically c...\n",
      "27    Amivantamab is an epidermal growth factor rece...\n",
      "28    The risk of treatment-related toxicities with ...\n",
      "29    Surgical resection is the best-known approach ...\n",
      "30    Being implicated in bone metastasis developmen...\n",
      "31    To explore how non-surgical esophageal cancer ...\n",
      "32    Epidermal growth factor receptor (EGFR) tyrosi...\n",
      "33    Lung cancer in Australia contributes 9% of all...\n",
      "34    Breast tissue markers are essential in localis...\n",
      "35    Colorectal cancer is the 3rd most common cance...\n",
      "36    Epidermal growth factor receptor (EGFR) kinase...\n",
      "37    At JADPRO Live 2023, presenters discussed the ...\n",
      "38    Dishevelled, EGL-10 and pleckstrin domain-cont...\n",
      "39    Ga-68 labeled prostate-specific membrane antig...\n",
      "40    Direct comparisons between 18 Fifty-one patien...\n",
      "41    This was a multicenter, single-arm dose-rangin...\n",
      "42    Cutaneous malignant melanoma is one of the mos...\n",
      "43    The effect of cancer in women has varied effec...\n",
      "44    Capmatinib is a selective MET inhibitor with d...\n",
      "45    As more treatments emerge for advanced, stage ...\n",
      "46    Immunotherapy has revolutionized the managemen...\n",
      "47    The Onco Summit 2023: The Latin American (LATA...\n",
      "48    The advent of immunotherapy has greatly improv...\n",
      "49    Antrodia cinnamomea-derived sulfated polysacch...\n",
      "Name: Abstract, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 243.54 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 3697.70 examples/s]\n",
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [05:40<00:00, 16.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 340.2966, 'train_samples_per_second': 0.264, 'train_steps_per_second': 0.062, 'train_loss': 2.0035483950660344, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation: {'eval_loss': 2.1885786056518555, 'eval_runtime': 13.9675, 'eval_samples_per_second': 0.716, 'eval_steps_per_second': 0.143, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=1024)\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "df = pd.read_csv('Question_1.csv') \n",
    "\n",
    "# Extract the abstracts\n",
    "abstracts = df['Abstract']\n",
    "abstracts = abstracts[:50]\n",
    "print(abstracts)\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": abstracts})\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "input_ids = tokenized_dataset['input_ids']\n",
    "labels = input_ids.clone()\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)\n",
    "\n",
    "train_size = 30\n",
    "val_size = 10\n",
    "test_size = 10\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    tokenized_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Update Trainer with train and validation sets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,    # Training set\n",
    "    eval_dataset=val_dataset,       # Validation set for tuning during training\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test set evaluation:\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x00000156B93948E0>\n",
      "Treatment for lung cancer includes surgery, radiation therapy and chemotherapy. The role of radiation in the management of non-small-cell lung carcinoma. A multidisciplinary approach. Treatment strategies, the role and the value of the radiation oncologist. This article reviews the current state of lung-cancer treatment and discusses the indications, clinical role, and value for radiation oncologists. Our purpose is to provide a framework for multidisciplinary team (MDT) management for patients with lung cancers. We present a comprehensive review of treatment for non small cell carcinoma (NSCLC) in an MDT setting. Lung-care is a multidisciplinary effort that encompasses the patient, family, oncology,\n",
      "Treatment for lung cancer includes chemotherapy, radiation, and targeted therapy. The first-line treatment options for non-small-cell lung carcinoma include cisplatin-based doublet chemotherapy and platinum-free doublet therapy (i.e., carboplatin plus paclitaxel). Lung cancer is the leading cause of cancer-related deaths worldwide. There is a great need to develop new treatment strategies for patients with non small cell carcinoma. A number of targeted therapies have been approved for the treatment of non non malignant lung disease, including tyrosine kinase inhibitors for human epidermal growth factor receptor 2 (HER2) -positive and anaplastic lymphoma kinase (ALK) -rearranged lung cancers. These targeted agents have\n",
      "Treatment for lung cancer includes radiation therapy, chemotherapy, and targeted therapy. The current standard of care for patients with unresectable stage III non-small cell lung carcinoma (NSCLC) is chemotherapy combined with radiotherapy. Treatment strategies vary widely across institutions and are generally based on best available evidence. We aimed to conduct a systematic review of the evidence for the treatment of stage IIIB non small-cell carcinoma of lung (cStage IIIB NSCLC, stage IIIC disease). A systematic literature review was performed on PubMed and EMBASE databases and the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines were followed. Inclusion criteria were: (1)\n",
      "Treatment for lung cancer includes surgical resection, radiation therapy, chemotherapy, and targeted therapies. The current standard of care is surgical therapy followed by adjuvant chemotherapy. Final results of a phase III randomized trial of surgical versus medical therapy for patients with stage IIIA non-small cell lung carcinoma (NSCLC) demonstrate that surgery is superior to chemotherapy for the treatment of stage III NSCLC, with a survival advantage of approximately 2 years. This study is registered with ClinicalTrials.gov, number NCT00190830.\n",
      "Treatment for lung cancer includes surgery, radiation therapy and chemotherapy. The role of surgery in patients with brain metastases. Lung cancer is the leading cause of cancer death in both men and women. Treatment of brain metastatic cancer: A comprehensive review. We searched Pubmed, Embase and Cochrane Database of Systematic Reviews for relevant studies. Patients with lung cancers that are diagnosed with metastasis to the brain are usually treated with surgery for brain metastasis. All studies were graded according to a modified Sackett scale.\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)\n",
    "\n",
    "prompt = \"Treatment for lung cancer includes\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  \n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_length=128,\n",
    "    num_return_sequences=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Real-world data are limited for patients with ...\n",
      "1     Risk management and self-management strategies...\n",
      "2     Postoperative pancreatic fistula (POPF) contin...\n",
      "3     We aim to develop a predictive model for lymph...\n",
      "4     Breast cancer is the most common invasive canc...\n",
      "5     The titled molecule 2-Amino-N-(2-chloro-6-meth...\n",
      "6     In resource-limited settings, data regarding t...\n",
      "7     Chemotherapy in an integral part of cancer tre...\n",
      "8     STK11 germline pathogenic variants are typical...\n",
      "9                Background Methods Results Conclusions\n",
      "10             Introduction Methods Results Conclusions\n",
      "11    The presented study depicts the synthesis of 1...\n",
      "12    Breast cancer is one of the most common cancer...\n",
      "13    Breast cancer ranks as the second most prevale...\n",
      "14    As the spatial resolution of positron emission...\n",
      "15    We aimed to investigate the impact of social c...\n",
      "16    Breast cancer (BC) is the most frequently occu...\n",
      "17    Breast Cancer (BC) poses significant challenge...\n",
      "18    Breast cancer is the most frequently diagnosed...\n",
      "19    Approximately 10 to 15% of patients with breas...\n",
      "20    Breast cancer has the potential to metastasize...\n",
      "21    One of the main regulators in the cell cycle i...\n",
      "22    A century ago, the first description of second...\n",
      "23    Progressive multifocal leukoencephalopathy (PM...\n",
      "24    A high abundance of Epidermal Growth Factor Re...\n",
      "25    In patients vulnerable to skeletal-related eve...\n",
      "26    Arytenoid dislocation, typically manifested as...\n",
      "27    Radiotherapy (RT) serves as one of the key adj...\n",
      "28    Breast cancer is one of the main Cancers affec...\n",
      "29    To find highly effective and low-toxicity anti...\n",
      "30    Background Objectives: Methods: Results: 2 2 C...\n",
      "31    Loop ileostomy is commonly performed by colore...\n",
      "32    There are limited real-world data (RWD) regard...\n",
      "33    Prophylactic dissection of paratracheal lymph ...\n",
      "34    To evaluate the efficacy and safety of four te...\n",
      "35    Cancer is a complex pathological condition ass...\n",
      "36    In the current study, we have investigated the...\n",
      "37    Globally, hormone-responsive cancers afflict m...\n",
      "38    Modern cancer therapies greatly improve clinic...\n",
      "39    Breast reconstruction after mastectomy helps w...\n",
      "40    Image-guided biopsy is an integral step in the...\n",
      "41    Surgical resection is the best-known approach ...\n",
      "42    Image-based data mining (IBDM) requires spatia...\n",
      "43    Being implicated in bone metastasis developmen...\n",
      "44    Objective: Methods: Results: P P Conclusion: Â©...\n",
      "45    This study aimed to assess the targeted nano-t...\n",
      "46    Radiation therapy plays a critical role in the...\n",
      "47    Breast tissue markers are essential in localis...\n",
      "48    There are limited options for primary preventi...\n",
      "49    Dishevelled, EGL-10 and pleckstrin domain-cont...\n",
      "Name: Abstract, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 274.74 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 4165.73 examples/s]\n",
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [06:14<00:00, 17.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 374.4316, 'train_samples_per_second': 0.24, 'train_steps_per_second': 0.056, 'train_loss': 2.340392521449498, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:23<00:00, 11.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation: {'eval_loss': 2.2494518756866455, 'eval_runtime': 23.9823, 'eval_samples_per_second': 0.417, 'eval_steps_per_second': 0.083, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=1024)\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "df = pd.read_csv('Question_2.csv') \n",
    "\n",
    "# Extract the abstracts\n",
    "abstracts = df['Abstract']\n",
    "abstracts = abstracts[:50]\n",
    "print(abstracts)\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": abstracts})\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "input_ids = tokenized_dataset['input_ids']\n",
    "labels = input_ids.clone()\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)\n",
    "\n",
    "train_size = 30\n",
    "val_size = 10\n",
    "test_size = 10\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    tokenized_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Update Trainer with train and validation sets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,    # Training set\n",
    "    eval_dataset=val_dataset,       # Validation set for tuning during training\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test set evaluation:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x00000157017E7A30>\n",
      "Treatment for breast cancer includes surgery, chemotherapy and / or radiation, and is usually administered over a course of several weeks. (1) Despite being the leading cause of cancer death in women worldwide, the management of breast cancers has evolved dramatically over the past decades. The development of targeted therapy has improved the survival of patients with breast carcinoma; however, it has also increased the risk of drug-related adverse events (ADRs), which can limit the use of these drugs and cause unnecessary side effects. In this context, understanding the causes of ADRs is crucial for the development and clinical use, as well as for improving drug safety. We conducted\n",
      "Treatment for breast cancer includes surgery, chemotherapy, and hormone therapy. An understanding of breast anatomy and the breast and its surrounding structures is essential for surgical planning. The aim of this study was to provide a comprehensive review of the literature on the anatomy of and techniques used for the treatment of benign and malignant breast diseases. This review was conducted using PubMed, EMBASE, Scopus, Web of Science, the Cochrane Library, CINAHL, Google Scholar, EBSCO, Medline Complete, Embase Classic, ERIC, Science Direct, ProQuest, Springer, Wiley, Physiotherapy Evidence Database, Cochrane Database of Systematic Reviews, SCOPUS, I\n",
      "Treatment for breast cancer includes local and / or systemic therapy. The use of systemic therapies has been increasing over the past decades; however, most of the available drugs have significant side effects. Based on the high prevalence of side-effects, it is important to identify and prioritize the most relevant side effect of each drug. We aimed to develop an algorithm to prioritize side impact of drugs used for the treatment of breast cancers. Thus, the main objective of this study was to construct a side Impact score (SIS) for each breast drug and its associated comorbidities. METHODS: We performed a literature review for side consequences of approved drugs for treatment breast-\n",
      "Treatment for breast cancer includes surgery, radiation therapy, chemotherapy, and hormonal therapy. The breast is a complex structure and may be affected by radiation and chemotherapy in combination or sequentially. As a result, the radiation treatment has to be adapted to the individual patient. This paper provides a detailed overview of the treatment planning and delivery of breast radiation, including the physical principles, dose calculation, clinical applications, treatment verification, patient positioning, image guidance, quality assurance, planning, radiotherapy delivery, normal tissue toxicity, toxicity management, as well as follow-up. In addition, this paper also provides practical advice on how to ensure that all these aspects are\n",
      "Treatment for breast cancer includes breast-conserving surgery (BCS), breast radiotherapy (RT) and endocrine therapy. The role of adjuvant systemic therapy has been controversial for decades. However, the recent introduction of targeted therapy, such as tyrosine kinase inhibitors (TKIs) for HER2-positive patients, has changed the treatment paradigm. In this review, we provide an overview of the clinical trials and the most recent data on the role and safety of systemic therapies, with a focus on TKIs. A comprehensive search was conducted using PubMed and Embase. We included clinical studies published in the English language in which systemic treatment was performed for the prevention or treatment of breast\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)\n",
    "\n",
    "prompt = \"Treatment for breast cancer includes\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  \n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_length=128,\n",
    "    num_return_sequences=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Pediatric neurosurgical practice is prevalent ...\n",
      "1     Posttraumatic osteoarthritis is a common indic...\n",
      "2     Delay in diagnosis and treatment of lung cance...\n",
      "3     Optical imaging is a powerful tool for early d...\n",
      "4     Successful acute migraine treatment potentiall...\n",
      "5     Masses in the forefoot and midfoot are common ...\n",
      "6     Acute kidney injury (AKI) is a prevalent compl...\n",
      "7     Despite the significant decline in the inciden...\n",
      "8     Neoadjuvant endocrine therapy presents an impo...\n",
      "9     Auxiliary diagnosis of different types of cyst...\n",
      "10    Osteoarthritis is more prevalent and severe am...\n",
      "11    The integration of positron emission tomograph...\n",
      "12    With the advent of PET imaging in 1976, 2-deox...\n",
      "13    Alzheimer's disease (AD) is the most common ne...\n",
      "14    Despite an increase in maternal prenatal canna...\n",
      "15    There is an unmet need for effective topical t...\n",
      "16    Attention Deficit Hyperactivity Disorder (ADHD...\n",
      "17    Gastric cancer (GC) is one of the most common ...\n",
      "18    Mucinous adenocarcinoma of anus is an uncommon...\n",
      "19    Chronotype is an inherent physiological trait ...\n",
      "20    Many consider white matter hyperintensities (W...\n",
      "21    Growing skull fracture (GSF) of the orbital ro...\n",
      "22    Cerebral aneurysms are diagnosed in 1-5% of pe...\n",
      "23    To investigate the effect of two fractionation...\n",
      "24    Exercise-induced bronchoconstriction (EIB) is ...\n",
      "25    Drug Reaction with Eosinophilia and Systemic S...\n",
      "26    Cervical cancer screening is a cornerstone of ...\n",
      "27    Cardiac amyloidosis (CA) is characterized by d...\n",
      "28    This study aimed to analyze the demographic ch...\n",
      "29    Blastomycosis is an environmentally acquired f...\n",
      "30    Acute kidney injury (AKI) and chronic kidney d...\n",
      "31    Adverse childhood experiences (ACEs) have long...\n",
      "32    Reductions in all-cause otitis media (OM) foll...\n",
      "33    Cancer will affect more than one in three U.S....\n",
      "34    Renal cell carcinoma (RCC) is a highly aggress...\n",
      "35    We demonstrate the peculiarities of diagnosis ...\n",
      "36    A rare case of thyroid foreign body is present...\n",
      "37    To create the prognostic scale based on some b...\n",
      "38    Thyroid cancer is the most prevalent form of e...\n",
      "39    In this study, we aimed to identify bone miner...\n",
      "40    The early detection and accurate monitoring of...\n",
      "41    This study aimed to analyze the prevalence and...\n",
      "42    The glaucoma treatment paradigm is starting to...\n",
      "43    Hundreds of thousands of children are victims ...\n",
      "44    Despite a higher rate of breast cancer in sub-...\n",
      "45    Psoriatic arthritis (PsA) is a heterogeneous, ...\n",
      "46    Optical coherence tomography angiography (OCTA...\n",
      "47    Surgical wound infection is a global postopera...\n",
      "48    Stroke poses a significant global health threa...\n",
      "49    Primary gastric small cell carcinoma (GSCC) is...\n",
      "Name: Abstract, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 241.58 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 3845.23 examples/s]\n",
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [07:50<00:00, 22.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 470.258, 'train_samples_per_second': 0.191, 'train_steps_per_second': 0.045, 'train_loss': 2.1990267435709634, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:15<00:00,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation: {'eval_loss': 2.397444725036621, 'eval_runtime': 15.2271, 'eval_samples_per_second': 0.657, 'eval_steps_per_second': 0.131, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=1024)\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "df = pd.read_csv('Question_3.csv') \n",
    "\n",
    "# Extract the abstracts\n",
    "abstracts = df['Abstract']\n",
    "abstracts = abstracts[:50]\n",
    "print(abstracts)\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": abstracts})\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "input_ids = tokenized_dataset['input_ids']\n",
    "labels = input_ids.clone()\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)\n",
    "\n",
    "train_size = 30\n",
    "val_size = 10\n",
    "test_size = 10\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    tokenized_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Update Trainer with train and validation sets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,    # Training set\n",
    "    eval_dataset=val_dataset,       # Validation set for tuning during training\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test set evaluation:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x000001571816AE60>\n",
      "Symptoms for lung cancer include cough, dyspnea, hemoptysis, chest pain, and chest tightness. The purpose of this study was to investigate the impact of lung-cancer-specific symptoms on quality of life (QOL) of patients and caregivers. A survey was conducted among patients with lung cancers and their caregivers in China. Patients were administered questionnaires to collect socio-demographic data and lung symptoms, including cough and dyspnea. Caregivers were also administered the same questionnaire. QOL was evaluated using the lung Cancer Symptom Scale (LCSS). Multiple regression analysis was used to identify factors that influence patients' QOL and caregiver QOL. Results: A total of 200\n",
      "Symptoms for lung cancer include dyspnea, chest pain, cough, and hemoptysis. The diagnosis of lung carcinoma typically requires advanced imaging techniques, such as computed tomography, which are usually performed in combination with tissue biopsy. Lung cancer can be detected in early stages, but imaging often lacks specificity. Therefore, the diagnostic accuracy of noninvasive techniques for early lung malignancy is limited. Herein, we present the case of a 79-year-old female patient with cough and dyspnea of two months' duration. Chest imaging revealed a mass in the right lower lobe (LLL). Surgical resection was performed and histopathological examination of the LLM revealed lung adenocarcinoma. Immunohistochemical\n",
      "Symptoms for lung cancer include cough, chest pain, hemoptysis, dyspnea, and cough-induced fever. The diagnosis of lung cancers requires a high degree of clinical suspicion and is often difficult to confirm. Early diagnosis is essential to improve the prognosis of patients. Here, we report the case of a patient with lung adenocarcinoma presenting as a persistent cough and a history of chest trauma. He was successfully treated with a combination of surgical resection and chemotherapy. To the best of our knowledge, this is the first report of an association between lung carcinoma and chest injury. Our findings suggest that clinicians should be vigilant for the possibility of underlying chest injuries in\n",
      "Symptoms for lung cancer include cough, shortness of breath, chest pain, hemoptysis, and chest tightness. The diagnosis of lung adenocarcinoma is often difficult, but the detection of specific proteins in blood can assist in the diagnosis. Lung cancer biomarkers are useful for diagnosis and early treatment. Blood biomarkers can also be used to assess the efficacy of therapy and monitor disease progression. However, current blood biomarkers for cancer have limitations, such as short half-life, poor specificity, or lack of sensitivity. To address these problems, several studies have attempted to develop blood-based biomarkers to improve early diagnosis, monitoring, treatment, prognosis, drug development, therapeutic\n",
      "Symptoms for lung cancer include cough, chest pain, shortness of breath, hemoptysis, and fever. The diagnosis of lung adenocarcinoma is usually confirmed based on histology, but other types of cancer can also mimic symptoms. However, the diagnostic workup is often challenging because of the similarities between symptoms of both diseases. Therefore, differential diagnosis is crucial for appropriate diagnosis. This article aims to provide an overview of common symptoms, diagnostic work-up, imaging, treatment, prognosis, follow-ups, or prognosis of patients with lung adenocarcinomas. Lung cancer is a heterogeneous disease, with symptoms ranging from benign to life-threatening. In the early stages, patients present\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)\n",
    "\n",
    "prompt = \"Symptoms for lung cancer include\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  \n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_length=128,\n",
    "    num_return_sequences=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Pediatric neurosurgical practice is prevalent ...\n",
      "1     Posttraumatic osteoarthritis is a common indic...\n",
      "2     Delay in diagnosis and treatment of lung cance...\n",
      "3     Paclitaxel-induced peripheral neuropathy (PN) ...\n",
      "4     Breast cancer is the leading cause of cancer-r...\n",
      "5     Optical imaging is a powerful tool for early d...\n",
      "6     The study objective was to determine the effec...\n",
      "7     Successful acute migraine treatment potentiall...\n",
      "8     Masses in the forefoot and midfoot are common ...\n",
      "9     Acute kidney injury (AKI) is a prevalent compl...\n",
      "10    Despite the significant decline in the inciden...\n",
      "11    Neoadjuvant endocrine therapy presents an impo...\n",
      "12    Auxiliary diagnosis of different types of cyst...\n",
      "13    Osteoarthritis is more prevalent and severe am...\n",
      "14    The integration of positron emission tomograph...\n",
      "15    With the advent of PET imaging in 1976, 2-deox...\n",
      "16    Alzheimer's disease (AD) is the most common ne...\n",
      "17    Despite an increase in maternal prenatal canna...\n",
      "18    There is an unmet need for effective topical t...\n",
      "19    Attention Deficit Hyperactivity Disorder (ADHD...\n",
      "20    Gastric cancer (GC) is one of the most common ...\n",
      "21    Mucinous adenocarcinoma of anus is an uncommon...\n",
      "22    Chronotype is an inherent physiological trait ...\n",
      "23    Many consider white matter hyperintensities (W...\n",
      "24    Growing skull fracture (GSF) of the orbital ro...\n",
      "25    Cerebral aneurysms are diagnosed in 1-5% of pe...\n",
      "26    To investigate the effect of two fractionation...\n",
      "27    This study was conducted to determine the leve...\n",
      "28    Exercise-induced bronchoconstriction (EIB) is ...\n",
      "29    Gut microbiota and associated metabolites have...\n",
      "30    Drug Reaction with Eosinophilia and Systemic S...\n",
      "31    Cervical cancer screening is a cornerstone of ...\n",
      "32    Cardiac amyloidosis (CA) is characterized by d...\n",
      "33    This study aimed to analyze the demographic ch...\n",
      "34    Blastomycosis is an environmentally acquired f...\n",
      "35    Acute kidney injury (AKI) and chronic kidney d...\n",
      "36    Adverse childhood experiences (ACEs) have long...\n",
      "37    Reductions in all-cause otitis media (OM) foll...\n",
      "38    Cancer will affect more than one in three U.S....\n",
      "39    Renal cell carcinoma (RCC) is a highly aggress...\n",
      "40    We demonstrate the peculiarities of diagnosis ...\n",
      "41    A rare case of thyroid foreign body is present...\n",
      "42    To study the diagnostic value of fluorescent l...\n",
      "43    To create the prognostic scale based on some b...\n",
      "44    In this study, we aimed to identify bone miner...\n",
      "45    The early detection and accurate monitoring of...\n",
      "46    This study aimed to analyze the prevalence and...\n",
      "47    The glaucoma treatment paradigm is starting to...\n",
      "48    Hundreds of thousands of children are victims ...\n",
      "49    Despite a higher rate of breast cancer in sub-...\n",
      "Name: Abstract, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 242.83 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 3845.23 examples/s]\n",
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [05:40<00:00, 16.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 340.3298, 'train_samples_per_second': 0.264, 'train_steps_per_second': 0.062, 'train_loss': 1.8107521420433408, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:20<00:00, 10.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation: {'eval_loss': 2.204596757888794, 'eval_runtime': 20.5334, 'eval_samples_per_second': 0.487, 'eval_steps_per_second': 0.097, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=1024)\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "df = pd.read_csv('Question_4.csv') \n",
    "\n",
    "# Extract the abstracts\n",
    "abstracts = df['Abstract']\n",
    "abstracts = abstracts[:50]\n",
    "print(abstracts)\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": abstracts})\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "input_ids = tokenized_dataset['input_ids']\n",
    "labels = input_ids.clone()\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)\n",
    "\n",
    "train_size = 30\n",
    "val_size = 10\n",
    "test_size = 10\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    tokenized_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Update Trainer with train and validation sets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,    # Training set\n",
    "    eval_dataset=val_dataset,       # Validation set for tuning during training\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test set evaluation:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x00000156B9395690>\n",
      "Symptoms for breast cancer include pain, swelling, and itching. A rare but serious complication of lymphedema is cellulitis, which can result in disfigurement and functional impairment. To our knowledge, this is the first report of cellulitis secondary to lymphedema after breast-conserving treatment. The patient was a 66-year-old woman with breast carcinoma who developed cellulitis after receiving adjuvant radiotherapy and chemotherapy. She had swelling and pain in the left axilla, resulting in considerable functional loss. After the cellulitis resolved, she underwent axillary lymph node dissection with axillary sentinel lymph nodes biopsy. Histopathological examination revealed that the lymph vessels were dilated and tortuous, with focal fibrinoid\n",
      "Symptoms for breast cancer include pain, bleeding, and swelling. The diagnosis of breast lump is often delayed, which can lead to an increased risk of local recurrence and distant metastasis. Early detection and prompt treatment can reduce the risk for late-stage breast cancers. Therefore, it is important to identify patients at risk, as early detection can improve outcomes. This study aimed to examine the association between self-reported symptoms and the diagnosis and treatment of early-onset breast lumps. Data were collected as part of the Australian Breast Cancer Family Registry (ABCFR), a prospective cohort study of Australian women diagnosed with breast Cancer in 2013-2016\n",
      "Symptoms for breast cancer include pain, swelling and tenderness. The pain can be caused by local infiltration of the affected area or metastasis. Most women with breast carcinoma are treated with surgery and / or radiotherapy. However, a considerable proportion of patients may develop local or distant recurrence. This can cause significant complications including lymphedema and skin complications such as skin burns and ulcerations. In this paper, we present the case of a patient with a local recurrence of breast adenocarcinoma treated successfully with mastectomy and adjuvant chemotherapy and radiotherapy, with no signs of local relapse or metastases at six months after treatment. Although the treatment of recurrent breast cancers is complex, early diagnosis of complications\n",
      "Symptoms for breast cancer include pain, itching, and bleeding. The current standard of care for the treatment of breast cancers is surgery and / or radiotherapy, but the use of chemotherapy and targeted therapies has been increasing over the past few decades. A number of cytotoxic agents, including anthracyclines, taxanes, platinum compounds, vinca alkaloids, cyclophosphamide, doxorubicin, fluorouracil, irinotecan, capecitabine, bevacizumab, trastuzumab, erlotinib, gefitinib, imatinib, ramucirumab, pemetrexed, paclitaxel, gemcitabine, docetaxel, vorinostat, topotecan, ixabepilone, rituximab, panitumumab and bevacizumab combined with targeted therapy, have been\n",
      "Symptoms for breast cancer include pain, bleeding, swelling, and pruritus. The diagnosis of breast carcinoma is often delayed, which may lead to severe morbidity. Early recognition of the disease and treatment can reduce the likelihood of delayed diagnosis. This article presents a case of a patient with breast pain that was initially misdiagnosed as cellulitis. After extensive review of her medical records, the authors discovered that the patient's pain was related to breast inflammation. She was later diagnosed with invasive ductal carcinoma. Breast pain may be the presenting symptom of an underlying breast malignancy. To the best of their knowledge, this is the first case report of invasive breast ductal adenocarcinoma presenting as\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)\n",
    "\n",
    "prompt = \"Symptoms for breast cancer include\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  \n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_length=128,\n",
    "    num_return_sequences=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Delay in diagnosis and treatment of lung cance...\n",
      "1     As catabolic enzyme, CD73 dephosphorylates ade...\n",
      "2     Autophagy serves as a critical regulator of im...\n",
      "3     This study aims to compare the cost-effectiven...\n",
      "4     4D cone-beam computed tomography (CBCT) plays ...\n",
      "5     Utidelone is an ebomycin derivative chemothera...\n",
      "6     The enhancer of rudimentary homolog (ERH) is s...\n",
      "7     Liver cancer (LC) is among the deadliest cance...\n",
      "8     Lung cancer remains the most prevalent maligna...\n",
      "9     Improvements in cancer control have led to a d...\n",
      "10    Non-small cell lung cancer (NSCLC) is the lead...\n",
      "11    Multiple microRNAs encapsulated in extracellul...\n",
      "12    Cisplatin resistance is common in nonâ€‘small ce...\n",
      "13    Cancer will affect more than one in three U.S....\n",
      "14    Thyroid cancer is the most prevalent form of e...\n",
      "15    With significant advancements in the study of ...\n",
      "16    Dysregulation of lung tissue collagen level pl...\n",
      "17    Primary gastric small cell carcinoma (GSCC) is...\n",
      "18    The primary goal of this research is to develo...\n",
      "19    Lung cancer is the most common type of cancer,...\n",
      "20    Several tyrosine kinase receptors inhibitors (...\n",
      "21    India is a vast and diverse country with exist...\n",
      "22    Radiotherapy (RT) is one of the primary treatm...\n",
      "23    Lung Cancer remains one of the leading causes ...\n",
      "24    Lung squamous cell carcinoma (LUSC) is the sec...\n",
      "25    Primary hepatoid adenocarcinoma of the lung (H...\n",
      "26    This study was designed to investigate the cli...\n",
      "27    Human epidermal growth factor receptor 2 (HER2...\n",
      "28    Lung cancer remains a leading cause of cancer-...\n",
      "29    To evaluate the prognostic impact of neutrophi...\n",
      "30    Volumetric modulated arc therapy (VMAT) is a n...\n",
      "31    To assess the molecular profile of follicular ...\n",
      "32    Human non-small cell lung cancer (NSCLC)is a v...\n",
      "33    The abnormality of the immune system caused by...\n",
      "34    This study aimed to conduct a systematic revie...\n",
      "35    Breast cancer with human epidermal growth fact...\n",
      "36    To explore the application value of a multimod...\n",
      "37    Dual bronchodilator therapy, consisting of a l...\n",
      "38    Low-fat dietary (LFD) pattern refers to a diet...\n",
      "39    In small cell lung cancer (SCLC), the patholog...\n",
      "40    The release of extracellular vesicles (EVs) in...\n",
      "41    The single-inhaler triple combination of beclo...\n",
      "42    Non-small cell lung cancer (NSCLC) is a highly...\n",
      "43    Initially, identified as a Hodgkin lymphoma ma...\n",
      "44    High heterogeneity in clinical benefit charact...\n",
      "45    A tendon's ordered extracellular matrix (ECM) ...\n",
      "46    This study explores the relationship between m...\n",
      "47    To study programmed death ligand 1 (PD-L1) exp...\n",
      "48    The plasma concentration of the macroautophagy...\n",
      "49    Non-small cell lung cancer (NSCLC) remains a l...\n",
      "Name: Abstract, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 284.53 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 3767.52 examples/s]\n",
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [09:33<00:00, 27.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 573.8683, 'train_samples_per_second': 0.157, 'train_steps_per_second': 0.037, 'train_loss': 2.098057156517392, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation: {'eval_loss': 1.9998762607574463, 'eval_runtime': 17.6431, 'eval_samples_per_second': 0.567, 'eval_steps_per_second': 0.113, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=1024)\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "df = pd.read_csv('Question_5.csv') \n",
    "\n",
    "# Extract the abstracts\n",
    "abstracts = df['Abstract']\n",
    "abstracts = abstracts[:50]\n",
    "print(abstracts)\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": abstracts})\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "input_ids = tokenized_dataset['input_ids']\n",
    "labels = input_ids.clone()\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)\n",
    "\n",
    "train_size = 30\n",
    "val_size = 10\n",
    "test_size = 10\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    tokenized_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Update Trainer with train and validation sets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,    # Training set\n",
    "    eval_dataset=val_dataset,       # Validation set for tuning during training\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test set evaluation:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x00000157B4A83790>\n",
      "Lung cancer is the leading cause of cancer-related deaths worldwide. A substantial number of lung cancer patients are diagnosed at an advanced stage of the disease, resulting in a poor prognosis. The standard of care for advanced non-small cell lung cancers (NSCLC) is platinum-based chemotherapy, with targeted therapies and immunotherapy being investigated for the treatment of advanced NSCLC. This review summarizes the state of knowledge regarding the use of targeted therapy, immunotherapy and combination therapies in advanced lung adenocarcinoma. We provide a comprehensive review of clinical trials investigating targeted agents, immunotherapies and combined treatments for lung adenocarcinomas. Expert opinion: The advent of new targeted drugs has changed the landscape\n",
      "Lung cancer is a leading cause of cancer-related deaths worldwide, with the majority of patients diagnosed in late stages. The diagnosis of lung cancer at an earlier stage can improve patient survival and reduce treatment-associated toxicities. Lung Cancer is the leading type of non-small cell lung carcinoma (NSCLC) and accounts for approximately 85% of all lung cancers diagnosed. In this study, we used a combination of proteomics and metabolomics to investigate the molecular changes induced by chemotherapy in the blood of mice bearing Lewis lung carcinomas (LLC). Male C57BL / 6 mice were inoculated with LLC cells, which were treated with either cisplatin or cisplatin plus docetaxel. Serum\n",
      "Lung cancer is the leading cause of cancer related deaths worldwide. The majority of lung cancer cases are diagnosed at an advanced stage with a poor prognosis. Lung adenocarcinoma (LUAD) is one of the most common types of non-small cell lung cancers. It is characterized by a unique genetic landscape, which can be targeted by targeted therapies. In recent years, several targeted therapeutics have been developed for the treatment of LUADs. However, the resistance of tumors to these therapies is still a major challenge. Here, we review the current targeted treatments for LUAD.We summarize the molecular mechanisms of resistance to targeted therapeutic agents. We also discuss the\n",
      "Lung cancer is the leading cause of cancer-related deaths worldwide. The lung cancer has unique characteristics that lead to a diverse range of clinical manifestations. This study aimed to characterize the clinical presentation and treatment outcomes in patients with lung adenocarcinoma harboring epidermal growth factor receptor (EGFR) mutations, compared with patients without EGFR mutations. We retrospectively analyzed patients diagnosed with primary lung adenocarcinomas between January 2014 and January 2019 in the Department of Thoracic Surgery, Cancer Hospital of Chinese Academy of Medical Sciences. Data on demographic characteristics, clinical symptoms, laboratory tests, treatment, and follow-up were collected. A total of 718 patients were included in this study. Of these\n",
      "Lung cancer is the leading cause of cancer-related deaths worldwide. The incidence of lung cancer has been increasing in China. Lung adenocarcinoma is an invasive type of non-small cell lung carcinoma, with a more aggressive and invasive growth pattern. In this study, we aimed to identify the potential targets of natural compounds in lung adenocarcinoma and explore the effects of the combination of two natural agents (curcumin and baicalin) on cell proliferation and apoptosis in human lung A549 cell lines. We used a combination index (CI) method to analyze the growth inhibition of A549 cells in the presence of different concentrations of curcumin, baicalein, or the two compounds\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)\n",
    "\n",
    "prompt = \"Lung cancer is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  \n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_length=128,\n",
    "    num_return_sequences=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Paclitaxel-induced peripheral neuropathy (PN) ...\n",
      "1     Breast cancer is the leading cause of cancer-r...\n",
      "2     The study objective was to determine the effec...\n",
      "3     One of the main health issues in the modern wo...\n",
      "4     There are currently no molecular tests to iden...\n",
      "5     Utidelone is an ebomycin derivative chemothera...\n",
      "6     Neoadjuvant endocrine therapy presents an impo...\n",
      "7     Social support has been linked to increased us...\n",
      "8     Sialyltransferases are enzymes that play a cru...\n",
      "9     Eribulin is an inhibitor of microtubule dynami...\n",
      "10    Benzofuropyridines (BFP) are polycyclic compou...\n",
      "11    This study was conducted to determine the leve...\n",
      "12    Improvements in cancer control have led to a d...\n",
      "13    Gut microbiota and associated metabolites have...\n",
      "14    Breast cancer (BC) remains a prevalent and cha...\n",
      "15    Differential RNA expression is becoming increa...\n",
      "16    Cancer will affect more than one in three U.S....\n",
      "17    Saltâ€‘induced kinase 1 (SIK1) is a serine/threo...\n",
      "18    To study the diagnostic value of fluorescent l...\n",
      "19    Breast Cancer stands on the second position in...\n",
      "20    With significant advancements in the study of ...\n",
      "21    In the present study, a novel series of sulfam...\n",
      "22    Despite a higher rate of breast cancer in sub-...\n",
      "23    We present the use of stimulated Brillouin sca...\n",
      "24    The relationship between the albumin-to-globul...\n",
      "25    Proteolysis-targeting chimeras (PROTACs) have ...\n",
      "26    India is a vast and diverse country with exist...\n",
      "27    Mindfulness-based stress reduction (MBSR) has ...\n",
      "28    The anti-angiogenic agent anlotinib offers a n...\n",
      "29    Randomised evidence supports the use of partia...\n",
      "30    Since 2020, breast cancer has remained the mos...\n",
      "31    In addition to blood test data, inflammation-b...\n",
      "32    Platypnea-orthodeoxia syndrome (POS) is an unc...\n",
      "33    Among the 7.8 million women with breast cancer...\n",
      "34    Breast cancer poses a significant global healt...\n",
      "35    Delays in initiating cancer treatment time to ...\n",
      "36    The Government of India (GOI) has launched a n...\n",
      "37    Vasculitides are a set of pathologies that can...\n",
      "38    Breast cancer is a common and complex disease,...\n",
      "39    [This corrects the article DOI: 10.3389/fnins....\n",
      "40    Breast cancer is the most common malignancy am...\n",
      "41    Neural cell adhesion molecule (NCAM), a common...\n",
      "42    Radiotherapy, a common cancer treatment, leads...\n",
      "43    The CRISPR-Cas systems are adopted as powerful...\n",
      "44    With the advancement of computer technology an...\n",
      "45    We examined neighborhood characteristics conce...\n",
      "46    Metastasis to the oral soft tissues and jaw is...\n",
      "47    Inflammatory breast carcinoma (IBC) is an aggr...\n",
      "48    We report the synthesis, characterization, ant...\n",
      "49    The integrated behaviour of multiple senescent...\n",
      "Name: Abstract, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 261.61 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 2324.85 examples/s]\n",
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [04:40<00:00, 13.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 280.142, 'train_samples_per_second': 0.321, 'train_steps_per_second': 0.075, 'train_loss': 2.1254348754882812, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation: {'eval_loss': 2.038769245147705, 'eval_runtime': 3.1513, 'eval_samples_per_second': 3.173, 'eval_steps_per_second': 0.635, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=1024)\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "df = pd.read_csv('Question_6.csv') \n",
    "\n",
    "# Extract the abstracts\n",
    "abstracts = df['Abstract']\n",
    "abstracts = abstracts[:50]\n",
    "print(abstracts)\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": abstracts})\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "input_ids = tokenized_dataset['input_ids']\n",
    "labels = input_ids.clone()\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)\n",
    "\n",
    "train_size = 30\n",
    "val_size = 10\n",
    "test_size = 10\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    tokenized_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Update Trainer with train and validation sets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,    # Training set\n",
    "    eval_dataset=val_dataset,       # Validation set for tuning during training\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test set evaluation:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x0000015718E2BF10>\n",
      "Breast cancer is the most common cancer affecting women worldwide. The incidence of this malignancy is increasing worldwide, with the highest incidence in developing countries. Despite significant advances in the diagnosis and treatment of breast cancer, the disease remains a major cause of mortality in women. New diagnostic and therapeutic methods are needed to improve the treatment outcomes for this cancer. In this review, we summarize the current advances and future prospects of non-invasive, real-time detection of circulating tumor cells (CTCs). We also discuss the clinical implications of CTCs in cancer treatment, including the use of cell-free DNA (cfDNA) as a biomarker for monitoring treatment response,\n",
      "Breast cancer is the most common malignancy among women and the second leading cause of cancer-related death in women worldwide. The present study aimed to investigate the association between serum vitamin D levels and breast cancer among patients. A total of 140 patients with breast carcinoma and 140 healthy controls were included in the present case-control study. Vitamin D status was assessed using an enzyme-linked immunosorbent assay (ELISA). No significant differences were observed between the two groups in terms of age, body mass index (BMI), tumor size, or the number of lymph nodes. However, the vitamin A level was significantly higher in patients compared with controls (P\n",
      "Breast cancer is a leading cause of cancer-related deaths in the world, and the incidence is increasing. A major cause for this increase is the poor understanding of the molecular mechanisms underlying breast cancer progression. The development of targeted therapies for breast cancers has resulted in increased patient survival; however, these therapies have failed to significantly improve the survival of patients with triple-negative breast carcinomas. Recent research has identified a number of important driver genes involved in breast carcinogenesis, which may serve as potential biomarkers for predicting the response to targeted therapy. However, the clinical significance of these biomarkers is unclear. To investigate the prognostic value of several driver gene mutations in patients\n",
      "Breast cancer is a common cancer in women worldwide. The 5-year survival rate is 80%. This study aimed to determine the association between breast cancer risk perception and the risk of breast invasive cancer. A total of 2022 women aged 18 years and above were enrolled in this study. Data were collected through a self-administered questionnaire including sociodemographic variables, risk perceptions, and breast screening. Breast Cancer Risk Perception Scale-A (BCRS-As) was used to assess the participants' risk level. Descriptive statistics were used for data analysis, including mean, standard deviation, median, interquartile range, frequency, percentage, percentages, means,\n",
      "Breast cancer is a complex disease that is characterized by the presence of heterogeneous tumor subtypes. A growing body of evidence demonstrates that epigenetic processes, including DNA methylation and histone modifications, play a role in breast cancer progression. DNA hypermethylation is associated with gene silencing, and epigenetic alterations are implicated in several human cancers, in particular in the regulation of the expression of genes encoding proteins involved in tumor progression and metastasis. Aberrant histone methylation is an epigenetic mechanism that regulates the gene expression, leading to an increase in gene transcription. Histone modifications are reversible, with histone methyltransferases (HMT) and demethylases (HDMs) being key regulators of these processes.\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)\n",
    "\n",
    "prompt = \"Breast cancer is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  \n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_length=128,\n",
    "    num_return_sequences=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Pancreatic adenosquamous carcinoma (PASC) is a...\n",
      "1     Sonodynamic therapy (SDT) is a promising strat...\n",
      "2     Pancreatic ductal adenocarcinoma (PDAC), the m...\n",
      "3     Pancreatoduodenectomy (PD) is a highly complex...\n",
      "4     V-domain Imuunoglobulin suppressor of T-cell a...\n",
      "5     Anaplastic thyroid cancer (ATC) is one of the ...\n",
      "6     Several tyrosine kinase receptors inhibitors (...\n",
      "7     Circulating tumor cells (CTCs) are tumor cells...\n",
      "8     Indoleamine 2,3-dioxygenase 1 (IDO1) plays a k...\n",
      "9     Tumor microenvironment (TME) is essential for ...\n",
      "10    Thromboembolic events (TEEs) are frequent amon...\n",
      "11    The aggressiveness of pancreatic ductal adenoc...\n",
      "12    Disconnected pancreatic duct syndrome (DPDS) i...\n",
      "13    Pancreatic ductal adenocarcinoma (PDAC) poses ...\n",
      "14    The purpose of this study was to translate the...\n",
      "15    Immunotherapy has made significant strides in ...\n",
      "16    Type 1 diabetes (T1D) is characterized by an a...\n",
      "17    Ubiquitination is one of the important modific...\n",
      "18    This meta-analysis aimed to compare the effica...\n",
      "19    Robotic surgery has demonstrated outcomes comp...\n",
      "20    Risk factors for pancreatic ductal adenocarcin...\n",
      "21    CD200, a negative regulator of T cells as well...\n",
      "22    To investigate the practices of clinicians pre...\n",
      "23    Surgical resection is the standard recommended...\n",
      "24    Enteroviruses, which infect via the gut, have ...\n",
      "25    Colorectal cancer (CRC), ranked as the globe's...\n",
      "26    L-type amino acid transporter 1 (LAT1) is a me...\n",
      "27    Pancreatic cancer, characterized by a dismal p...\n",
      "28    The peptidyl-prolyl cis/trans isomerase NIMA-i...\n",
      "29    The incidence of GI cancers is increasing in s...\n",
      "30    This study aims to clarify the benefit of adju...\n",
      "31    Porcine cancer models offer a valuable platfor...\n",
      "32    Despite the presence of various guidelines, di...\n",
      "33    Data on the effects of glucagon-like peptide-1...\n",
      "34    Associations of body mass index (BMI) with sur...\n",
      "35    Combined pancreatic and vascular resections ar...\n",
      "36    Chimeric antigen receptor (CAR) T-cell therapy...\n",
      "37    KRAS mutations occur in ~40-50% of mCRC and ar...\n",
      "38    Pancreatic cancer is an aggressive and metasta...\n",
      "39    Pancreatic cancer is an aggressive cancer with...\n",
      "40    Pancreatic ductal adenocarcinoma (PDAC) patien...\n",
      "41    Immunogenic cell death (ICD) can be exploited ...\n",
      "42    The outcome of clinical trials evaluating drug...\n",
      "43    Several studies have evaluated the impact of s...\n",
      "44    Pancreatic cancer (PC) is a highly aggressive ...\n",
      "45    Pancreatic cancer is usually detected through ...\n",
      "46    Homologous recombination (HR) is an important ...\n",
      "47    Organs in the abdominal cavity are covered by ...\n",
      "48    Immunotherapy is an emerging strategy in cance...\n",
      "49    The discovery of embryonic stem cell (ESC) med...\n",
      "Name: Abstract, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 228.92 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 3124.39 examples/s]\n",
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [07:49<00:00, 67.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 469.9399, 'train_samples_per_second': 0.064, 'train_steps_per_second': 0.015, 'train_loss': 2.2471866607666016, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation: {'eval_loss': 1.9083530902862549, 'eval_runtime': 207.2702, 'eval_samples_per_second': 0.048, 'eval_steps_per_second': 0.01, 'epoch': 0.9333333333333333}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=1024)\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "df = pd.read_csv('Question_7.csv') \n",
    "\n",
    "# Extract the abstracts\n",
    "abstracts = df['Abstract']\n",
    "abstracts = abstracts[:50]\n",
    "print(abstracts)\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": abstracts})\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "input_ids = tokenized_dataset['input_ids']\n",
    "labels = input_ids.clone()\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)\n",
    "\n",
    "train_size = 30\n",
    "val_size = 10\n",
    "test_size = 10\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    tokenized_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Update Trainer with train and validation sets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,    # Training set\n",
    "    eval_dataset=val_dataset,       # Validation set for tuning during training\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test set evaluation:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x0000013A3AA92DD0>\n",
      "Pancreatic cancer is a leading cause of cancer-related death worldwide. The aim of this study was to investigate the prognostic role of the combination of neutrophil-to-lymphocyte ratio (NLR) and platelet-monocyte ratio in patients with pancreatic cancer. A total of 621 patients who underwent curative resection for pancreatic ductal adenocarcinoma between January 2010 and December 2015 were included in the study. Univariate and multivariate Cox proportional hazards models were used to determine the association between the NLR-platelet monocyte index (PMI) ratio and clinical outcomes. An NLR > 3 and a PMI > 12 were identified as independent risk factors for poor overall survival (OS). In the\n",
      "Pancreatic cancer is one of the most aggressive and lethal cancers. The prognosis is poor due to late diagnosis, high recurrence rate, and the lack of effective treatment strategies. In recent years, great efforts have been made to identify biomarkers that can predict the prognosis of pancreatic cancer. However, most of these biomarkers are still under development. Although several novel biomarkers have recently been identified, the predictive value of each biomarker remains to be validated. Recently, several biomarker-based clinical trials have investigated the efficacy of targeted therapies and immunotherapies in patients with pancreatic ductal adenocarcinoma. This article summarizes the current progress in biomarker research in pancreatic carcinoma and discusses the future direction of\n",
      "Pancreatic cancer is one of the most aggressive cancers, and is associated with a poor prognosis. This study aimed to analyze the prognostic effect of tumor-infiltrating lymphocytes (TILs) in pancreatic cancer. A meta-analysis was performed to evaluate the association between TILs and survival outcomes. PubMed, Embase, Web of Science, EBSCO, the Cochrane Library, Wanfang Data, CNKI, CBM, VIP, PubMed and the China Biology Medicine disc (CBMM) were systematically searched to identify eligible studies. Hazard ratios (HRs) and 95% confidence intervals (CIs) for overall survival (OS), disease-free survival and\n",
      "Pancreatic cancer is one of the leading causes of cancer-related deaths worldwide.\n",
      "Pancreatic cancer is one of the most aggressive malignancies with poor prognosis. This study aimed to identify a gene signature that can be used to predict the prognosis of patients with pancreatic cancer (PC). We identified differentially expressed genes (DEGs) between PC tissues and normal pancreatic tissues from the Cancer Genome Atlas database. A risk score was calculated based on the expression of these DEGs. The risk scoring system was used for survival analysis. Functional enrichment analysis of DEGs was performed, and the protein-protein interaction (PPI) network was constructed. Additionally, the prognostic signature was established. Univariate Cox regression analysis and multivariate Cox proportional hazard analysis were used in the construction\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)\n",
    "\n",
    "prompt = \"Pancreatic cancer is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  \n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_length=128,\n",
    "    num_return_sequences=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Pediatric neurosurgical practice is prevalent ...\n",
      "1     Posttraumatic osteoarthritis is a common indic...\n",
      "2     Delay in diagnosis and treatment of lung cance...\n",
      "3     Optical imaging is a powerful tool for early d...\n",
      "4     Successful acute migraine treatment potentiall...\n",
      "5     Masses in the forefoot and midfoot are common ...\n",
      "6     Acute kidney injury (AKI) is a prevalent compl...\n",
      "7     Despite the significant decline in the inciden...\n",
      "8     Neoadjuvant endocrine therapy presents an impo...\n",
      "9     Auxiliary diagnosis of different types of cyst...\n",
      "10    Osteoarthritis is more prevalent and severe am...\n",
      "11    The integration of positron emission tomograph...\n",
      "12    With the advent of PET imaging in 1976, 2-deox...\n",
      "13    Alzheimer's disease (AD) is the most common ne...\n",
      "14    Despite an increase in maternal prenatal canna...\n",
      "15    There is an unmet need for effective topical t...\n",
      "16    Pancreatic adenosquamous carcinoma (PASC) is a...\n",
      "17    Attention Deficit Hyperactivity Disorder (ADHD...\n",
      "18    Gastric cancer (GC) is one of the most common ...\n",
      "19    Mucinous adenocarcinoma of anus is an uncommon...\n",
      "20    Chronotype is an inherent physiological trait ...\n",
      "21    Many consider white matter hyperintensities (W...\n",
      "22    Growing skull fracture (GSF) of the orbital ro...\n",
      "23    Cerebral aneurysms are diagnosed in 1-5% of pe...\n",
      "24    To investigate the effect of two fractionation...\n",
      "25    Exercise-induced bronchoconstriction (EIB) is ...\n",
      "26    Drug Reaction with Eosinophilia and Systemic S...\n",
      "27    V-domain Imuunoglobulin suppressor of T-cell a...\n",
      "28    Cervical cancer screening is a cornerstone of ...\n",
      "29    Cardiac amyloidosis (CA) is characterized by d...\n",
      "30    This study aimed to analyze the demographic ch...\n",
      "31    Blastomycosis is an environmentally acquired f...\n",
      "32    Acute kidney injury (AKI) and chronic kidney d...\n",
      "33    Adverse childhood experiences (ACEs) have long...\n",
      "34    Reductions in all-cause otitis media (OM) foll...\n",
      "35    Renal cell carcinoma (RCC) is a highly aggress...\n",
      "36    We demonstrate the peculiarities of diagnosis ...\n",
      "37    A rare case of thyroid foreign body is present...\n",
      "38    To create the prognostic scale based on some b...\n",
      "39    In this study, we aimed to identify bone miner...\n",
      "40    The early detection and accurate monitoring of...\n",
      "41    This study aimed to analyze the prevalence and...\n",
      "42    The glaucoma treatment paradigm is starting to...\n",
      "43    Hundreds of thousands of children are victims ...\n",
      "44    Despite a higher rate of breast cancer in sub-...\n",
      "45    Psoriatic arthritis (PsA) is a heterogeneous, ...\n",
      "46    Optical coherence tomography angiography (OCTA...\n",
      "47    Surgical wound infection is a global postopera...\n",
      "48    Stroke poses a significant global health threa...\n",
      "49    Primary gastric small cell carcinoma (GSCC) is...\n",
      "Name: Abstract, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 170.33 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 2695.71 examples/s]\n",
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [23:19<?, ?it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [09:28<00:00, 27.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 568.234, 'train_samples_per_second': 0.158, 'train_steps_per_second': 0.037, 'train_loss': 2.105058034261068, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:26<00:00, 13.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation: {'eval_loss': 2.4211714267730713, 'eval_runtime': 27.2083, 'eval_samples_per_second': 0.368, 'eval_steps_per_second': 0.074, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=1024)\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "df = pd.read_csv('Question_8.csv') \n",
    "\n",
    "# Extract the abstracts\n",
    "abstracts = df['Abstract']\n",
    "abstracts = abstracts[:50]\n",
    "print(abstracts)\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": abstracts})\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "input_ids = tokenized_dataset['input_ids']\n",
    "labels = input_ids.clone()\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)\n",
    "\n",
    "train_size = 30\n",
    "val_size = 10\n",
    "test_size = 10\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    tokenized_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Update Trainer with train and validation sets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,    # Training set\n",
    "    eval_dataset=val_dataset,       # Validation set for tuning during training\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test set evaluation:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x0000013873CB91B0>\n",
      "Symptoms for pancreatic cancer include pain, jaundice, and weight loss. The incidence of pancreatic ductal adenocarcinoma (PDAC) is increasing, which requires early diagnosis. However, the lack of early diagnostic tools for PDAC has led to a high misdiagnosis rate. In this study, we developed a novel biomarker panel based on the combination of four proteins (FAM187A, CASP3, TGFBRAP1, CCDC42) for the early detection of PDAC. We used the Luminex xMAP Â® platform to simultaneously quantify the four biomarkers in serum samples from 80 PDAC patients, 90 patients with chronic pancreatitis (CP), 87 patients without pancreatic disease\n",
      "Symptoms for pancreatic cancer include abdominal pain, jaundice, and weight loss. The diagnosis of pancreatic ductal adenocarcinoma (PDAC) is often challenging due to the nonspecific symptoms of the disease. In this study, we aimed to evaluate the diagnostic performance of contrast-enhanced CT (CECT) in patients with pancreatic lesions suspected of being PDAC. We retrospectively evaluated 484 patients who underwent CECTs for suspected pancreatic malignancies between January 2012 and September 2016. All patients underwent contrast enhanced MRI (CEMRI), CT, or both. Pancreatic lesions were categorized as benign or malignant based on MRI or CT findings, respectively. MRI and CT were performed in all\n",
      "Symptoms for pancreatic cancer include abdominal pain, weight loss, and jaundice. A case of adenocarcinoma of the pancreas that metastasized to the spleen and the liver is presented. The patient had symptoms of pancreatic head cancer. He was treated with a combination of gemcitabine and docetaxel. After the second cycle, he complained of epigastric pain and a palpable mass in the epigastrium. Abdominal ultrasonography revealed a large mass with irregular margins in his spleen. Magnetic resonance cholangiopancreatography showed that the mass had invaded the pancreatic duct and liver parenchyma. Percutaneous biopsy of a mass revealed adenocarcinoma. Histopathological analysis of resected tissue showed pancreatic ductal adenocarcinoma, with metastasis to his liver and spleen\n",
      "Symptoms for pancreatic cancer include abdominal pain, jaundice, weight loss, and jaundice. The diagnosis of pancreatic ductal adenocarcinoma (PDAC) is usually made after imaging studies, but pancreatic cysts may be detected incidentally. It is not uncommon for physicians to encounter pancreatic cyst cases in patients with a pancreatic mass, which may lead to misdiagnosis. In this study, we aimed to investigate the diagnostic performance of computed tomography (CT) and magnetic resonance imaging (MRI) in pancreatic cystic lesions and to identify the predictors of malignancy. We retrospectively reviewed the medical records of patients who were diagnosed with pancreatic serous cystadenoma (PSC), mucinous cystadenomas (MCAs)\n",
      "Symptoms for pancreatic cancer include pain, jaundice, nausea and vomiting, and weight loss. The diagnosis is often delayed, which may lead to advanced disease. Early diagnosis and treatment may improve survival. In the absence of symptoms, cancer can be suspected based on the presence of elevated serum tumor markers or imaging. However, in the setting of atypical symptoms or elevated tumor marker levels, the diagnosis of pancreatic adenocarcinoma can present a diagnostic challenge. Serum tumor biomarkers, such as carbohydrate antigen 19-9 (CA 19 â€‘ 9), carcinoembryonic antigen (CEA) and carbohydrate antigen72-4 (CA72 â€‘ 4) are commonly used for diagnosis. Imaging\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)\n",
    "\n",
    "prompt = \"Symptoms for pancreatic cancer include\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  \n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_length=128,\n",
    "    num_return_sequences=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     To investigate the practices of clinicians pre...\n",
      "1     Postoperative pancreatic fistula (POPF) contin...\n",
      "2     STK11 germline pathogenic variants are typical...\n",
      "3     The overall treatment response among patients ...\n",
      "4     Post-transplant HCC recurrence significantly i...\n",
      "5     Patients with synchronous pancreatic ductal ad...\n",
      "6     This research aimed to assess the value of rad...\n",
      "7     Pancreatic Ductal Adenocarcinoma (PDAC) primar...\n",
      "8     Muscular dystrophies and myotonic disorders ar...\n",
      "9     Tumor-infiltrating lymphocytes (TILs) are sign...\n",
      "10    Although atezolizumab plus bevacizumab (Atezo/...\n",
      "11    Cutaneous malignant melanoma is one of the mos...\n",
      "12    Pancreatic cancer, the 12th-most common cancer...\n",
      "13    Cancer immunotherapy using immune checkpoint i...\n",
      "14    Given the extensive role of lipids in cancer d...\n",
      "15    Pancreatic cancer is a prevalent malignant tum...\n",
      "16    Conducting\"health economic evaluation\"is one o...\n",
      "17    The prognosis of pancreatic cancer (PDAC) afte...\n",
      "18    The incidence of pancreatic cystic neoplasms i...\n",
      "19    Germline breast cancer susceptibility gene (gB...\n",
      "20    Single large hepatocellular carcinoma >5cm (SL...\n",
      "21    Lenvatinib (dosing for patients who weigh â‰¥60 ...\n",
      "22    To investigate in patients treated for a resec...\n",
      "23    Biomarkers for prediction of outcome in patien...\n",
      "24    This editorial discusses the article \"Analysis...\n",
      "25    Colon adenocarcinoma (COAD) is a prevalent mal...\n",
      "26    Hepatocellular carcinoma (HCC) is one of the m...\n",
      "27    Progressive functional decline is a key elemen...\n",
      "28    The prognosis of Klatskin tumors is poor, and ...\n",
      "29    Primary pancreatic lymphomas (PPLs) are a subg...\n",
      "30    Significant progress has been made in the mana...\n",
      "31    Neoadjuvant treatment (NAT) is standard for bo...\n",
      "32    Surgical resection (SR) is the main treatment ...\n",
      "33    Neuroendocrine neoplasms are a diverse group o...\n",
      "34    For reliable DCE MRI parameter estimation, k-s...\n",
      "35    The purpose of the study is to construct meani...\n",
      "36    Pancreatic ductal adenocarcinoma is an aggress...\n",
      "37    Both the incidence and prevalence of well-diff...\n",
      "38    Tumour morphology (tumour burden score (TBS)) ...\n",
      "39    We aimed to establish global benchmark outcome...\n",
      "40    Parenteral nutrition (PN) can be an effective ...\n",
      "41    Apart from cytotoxicity, inhibitors of the COX...\n",
      "42    To evaluate the quality of guidelines on the p...\n",
      "43    This is a case of a 67-year-old woman diagnose...\n",
      "44    Posthepatectomy liver failure (PHLF), complica...\n",
      "45    Developing clinically meaningful nanomedicines...\n",
      "46    This study was designed to explore and validat...\n",
      "47    To assess the utility of tumor-intrinsic and c...\n",
      "48    Gastroenteropancreatic neuroendocrine tumors (...\n",
      "49    Electron Paramagnetic Resonance Imaging (EPRI)...\n",
      "Name: Abstract, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 239.71 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 2217.28 examples/s]\n",
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [03:26<00:00,  9.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 206.4305, 'train_samples_per_second': 0.436, 'train_steps_per_second': 0.102, 'train_loss': 2.112316676548549, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:19<00:00,  9.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation: {'eval_loss': 2.2225022315979004, 'eval_runtime': 19.1673, 'eval_samples_per_second': 0.522, 'eval_steps_per_second': 0.104, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=1024)\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "df = pd.read_csv('Question_9.csv') \n",
    "\n",
    "# Extract the abstracts\n",
    "abstracts = df['Abstract']\n",
    "abstracts = abstracts[:50]\n",
    "print(abstracts)\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": abstracts})\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "input_ids = tokenized_dataset['input_ids']\n",
    "labels = input_ids.clone()\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)\n",
    "\n",
    "train_size = 30\n",
    "val_size = 10\n",
    "test_size = 10\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    tokenized_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Update Trainer with train and validation sets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,    # Training set\n",
    "    eval_dataset=val_dataset,       # Validation set for tuning during training\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test set evaluation:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x0000015178CE4700>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)  \n\u001b[0;32m      8\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {key: val\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m     22\u001b[0m     generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[i], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Almond\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Almond\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:1543\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1536\u001b[0m         input_ids,\n\u001b[0;32m   1537\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mnum_return_sequences,\n\u001b[0;32m   1538\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1540\u001b[0m     )\n\u001b[0;32m   1542\u001b[0m     \u001b[38;5;66;03m# 12. run sample\u001b[39;00m\n\u001b[1;32m-> 1543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m   1544\u001b[0m         input_ids,\n\u001b[0;32m   1545\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1546\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mlogits_warper,\n\u001b[0;32m   1547\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1548\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mpad_token_id,\n\u001b[0;32m   1549\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39meos_token_id,\n\u001b[0;32m   1550\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39moutput_scores,\n\u001b[0;32m   1551\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1552\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1553\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1554\u001b[0m     )\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_gen_mode:\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_return_sequences \u001b[38;5;241m>\u001b[39m num_beams:\n",
      "File \u001b[1;32mc:\\Users\\Almond\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:2482\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   2479\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2481\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2482\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[0;32m   2483\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2484\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2485\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   2486\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2487\u001b[0m )\n\u001b[0;32m   2489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2490\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Almond\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Almond\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1347\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[0;32m   1341\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[0;32m   1342\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1343\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1345\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1347\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1348\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1353\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1357\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1359\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1361\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1362\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\Almond\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Almond\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1014\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1005\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1007\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1008\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1009\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1027\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Almond\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Almond\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:603\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    594\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    595\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    596\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    600\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    601\u001b[0m     )\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 603\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Almond\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Almond\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:489\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    479\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    487\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    488\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    496\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Almond\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Almond\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:419\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    411\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    418\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 419\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    428\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    429\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Almond\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Almond\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:308\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    307\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(hidden_states))\n\u001b[1;32m--> 308\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    310\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder:\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;66;03m# if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;66;03m# Further calls to cross_attention layer can then reuse all cross-attention\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;66;03m# can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;66;03m# if encoder bi-directional self-attention `past_key_value` is always `None`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Almond\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Almond\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(test_dataset)\n",
    "\n",
    "prompt = \"Treatment for pancreatic cancer include\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  \n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_length=128,\n",
    "    num_return_sequences=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BioGptForCausalLM(\n",
       "  (biogpt): BioGptModel(\n",
       "    (embed_tokens): BioGptScaledWordEmbedding(42384, 1024, padding_idx=1)\n",
       "    (embed_positions): BioGptLearnedPositionalEmbedding(1026, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x BioGptDecoderLayer(\n",
       "        (self_attn): BioGptSdpaAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (activation_fn): GELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (output_projection): Linear(in_features=1024, out_features=42384, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "# from datasets import Dataset\n",
    "# import torch\n",
    "\n",
    "# tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "# model = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")\n",
    "# model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 231.73 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 2726.47 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 221.59 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 2999.36 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 200.66 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 3332.43 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 197.82 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 2788.58 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 232.32 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 3332.69 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 216.81 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 3526.80 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 204.75 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 3332.60 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 186.80 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 2726.94 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 195.31 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 3332.34 examples/s]\n",
      "C:\\Users\\Almond\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                      \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [04:30<00:00, 27.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 270.2112, 'train_samples_per_second': 0.148, 'train_steps_per_second': 0.037, 'train_loss': 1.9663129806518556, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:23<00:00, 11.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation: {'eval_loss': 2.1346869468688965, 'eval_runtime': 23.3001, 'eval_samples_per_second': 0.429, 'eval_steps_per_second': 0.086, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# from transformers import BioGptTokenizer, BioGptForCausalLM, Trainer, TrainingArguments\n",
    "# from datasets import Dataset, concatenate_datasets\n",
    "# import pandas as pd\n",
    "# from datasets import Dataset\n",
    "\n",
    "# tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "# q1 = pd.read_csv('Question_1.csv') \n",
    "# q2 = pd.read_csv('Question_2.csv')\n",
    "# q3 = pd.read_csv('Question_3.csv')\n",
    "# q4 = pd.read_csv('Question_4.csv')\n",
    "# q5 = pd.read_csv('Question_5.csv')\n",
    "# q6 = pd.read_csv('Question_6.csv')\n",
    "# q7 = pd.read_csv('Question_7.csv')\n",
    "# q8 = pd.read_csv('Question_8.csv')\n",
    "# q9 = pd.read_csv('Question_9.csv')\n",
    "\n",
    "# a1 = q1['Abstract'][:30]\n",
    "# a2 = q2['Abstract'][:30]\n",
    "# a3 = q3['Abstract'][:30]\n",
    "# a4 = q4['Abstract'][:30]\n",
    "# a5 = q5['Abstract'][:30]\n",
    "# a6 = q6['Abstract'][:30]\n",
    "# a7 = q7['Abstract'][:30]\n",
    "# a8 = q8['Abstract'][:30]\n",
    "# a9 = q9['Abstract'][:30]\n",
    "\n",
    "# def preprocess_function(examples):\n",
    "#     return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=1024)\n",
    "\n",
    "# def get_abstracts(abstracts):\n",
    "#     dataset = Dataset.from_dict({\"text\": abstracts})\n",
    "#     tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "#     tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "#     tokenized_dataset.set_format(\"torch\")\n",
    "#     tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "#     return tokenized_dataset\n",
    "\n",
    "# tokenized_dataset1 = get_abstracts(a1)\n",
    "# tokenized_dataset2 = get_abstracts(a2)\n",
    "# tokenized_dataset3 = get_abstracts(a3)\n",
    "# tokenized_dataset4 = get_abstracts(a4)\n",
    "# tokenized_dataset5 = get_abstracts(a5)\n",
    "# tokenized_dataset6 = get_abstracts(a6)\n",
    "# tokenized_dataset7 = get_abstracts(a7)\n",
    "# tokenized_dataset8 = get_abstracts(a8)\n",
    "# tokenized_dataset9 = get_abstracts(a9)\n",
    "\n",
    "# def subset_to_dataset(subset):\n",
    "#     return Dataset.from_dict(subset.dataset[subset.indices])\n",
    "\n",
    "# def get_datasets(tokenized_dataset):\n",
    "#     train_size = 20\n",
    "#     val_size = 5\n",
    "#     test_size = 5\n",
    "\n",
    "#     train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "#         tokenized_dataset, [train_size, val_size, test_size]\n",
    "#     )\n",
    "#     return subset_to_dataset(train_dataset), subset_to_dataset(val_dataset), subset_to_dataset(test_dataset)\n",
    "\n",
    "# a1_train_dataset, a1_val_dataset1, a1_test_dataset1 = get_datasets(tokenized_dataset1)\n",
    "# a2_train_dataset, a2_val_dataset2, a2_test_dataset2 = get_datasets(tokenized_dataset2)\n",
    "# a3_train_dataset, a3_val_dataset3, a3_test_dataset3 = get_datasets(tokenized_dataset3)\n",
    "# a4_train_dataset, a4_val_dataset4, a4_test_dataset4 = get_datasets(tokenized_dataset4)\n",
    "# a5_train_dataset, a5_val_dataset5, a5_test_dataset5 = get_datasets(tokenized_dataset5)\n",
    "# a6_train_dataset, a6_val_dataset6, a6_test_dataset6 = get_datasets(tokenized_dataset6)\n",
    "# a7_train_dataset, a7_val_dataset7, a7_test_dataset7 = get_datasets(tokenized_dataset7)\n",
    "# a8_train_dataset, a8_val_dataset8, a8_test_dataset8 = get_datasets(tokenized_dataset8)\n",
    "# a9_train_dataset, a9_val_dataset9, a9_test_dataset9 = get_datasets(tokenized_dataset9)\n",
    "\n",
    "\n",
    "# final_test_dataset = concatenate_datasets([a1_test_dataset1,  a7_test_dataset7])\n",
    "# final_train_dataset = concatenate_datasets([a1_train_dataset, a7_train_dataset])\n",
    "# final_val_dataset = concatenate_datasets([a1_val_dataset1, a7_val_dataset7])\n",
    "\n",
    "# # print(final_train_dataset)\n",
    "\n",
    "# input_ids = final_train_dataset['input_ids']\n",
    "# labels = input_ids\n",
    "\n",
    "# from transformers import DataCollatorForLanguageModeling\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# # Define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     learning_rate=2e-5,\n",
    "#     lr_scheduler_type=\"linear\",\n",
    "#     seed=42,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     num_train_epochs=1,\n",
    "#     weight_decay=0.01,\n",
    "#     gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "#     metric_for_best_model=\"loss\",\n",
    "#     greater_is_better=False,\n",
    "#     load_best_model_at_end=True,\n",
    "# )\n",
    "\n",
    "\n",
    "# # Update Trainer with train and validation sets\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=final_train_dataset,    # Training set\n",
    "#     eval_dataset=final_val_dataset,       # Validation set for tuning during training\n",
    "#     data_collator=data_collator,\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train()\n",
    "# metrics = trainer.evaluate(final_test_dataset)\n",
    "# print(\"Test set evaluation:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_text(prompt):\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model.to(device)  \n",
    "#     inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "#     outputs = model.generate(\n",
    "#         input_ids=inputs[\"input_ids\"],\n",
    "#         max_length=128,\n",
    "#         num_return_sequences=5, \n",
    "#         no_repeat_ngram_size=2, \n",
    "#         do_sample=True,\n",
    "#         top_k=50,\n",
    "#         top_p=0.95,\n",
    "#         temperature=0.7,\n",
    "#     )\n",
    "\n",
    "#     print('Prompt:', prompt)\n",
    "#     for i in range(5):\n",
    "#         generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "#         print(generated_text)\n",
    "\n",
    "\n",
    "# generate_text(\"Treatment for lung cancer includes\")\n",
    "# generate_text(\"Treatment for breast cancer includes\")\n",
    "# generate_text(\"Symptoms for lung cancer include\")\n",
    "# generate_text(\"Symptoms for breast cancer include\")\n",
    "# generate_text(\"Lung cancer is\")\n",
    "# generate_text(\"Breast cancer is\")\n",
    "# generate_text(\"Pancreatic cancer is\")\n",
    "# generate_text(\"Symptoms for pancreatic cancer include\")\n",
    "# generate_text(\"Treatment for pancreatic cancer include\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write the final dataset to a csv file\n",
    "# final_train_dataset.to_csv('final_train_dataset.csv')\n",
    "# final_val_dataset.to_csv('final_val_dataset.csv')\n",
    "# final_test_dataset.to_csv('final_test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Almond/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\Almond/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\Almond/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\\merges.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\Almond/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\Almond/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\Almond/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Assigning [PAD] to the pad_token key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Delay in diagnosis and treatment of lung cance...\n",
      "1     As catabolic enzyme, CD73 dephosphorylates ade...\n",
      "2     Autophagy serves as a critical regulator of im...\n",
      "3     This study aims to compare the cost-effectiven...\n",
      "4     4D cone-beam computed tomography (CBCT) plays ...\n",
      "5     Utidelone is an ebomycin derivative chemothera...\n",
      "6     The enhancer of rudimentary homolog (ERH) is s...\n",
      "7     Liver cancer (LC) is among the deadliest cance...\n",
      "8     Lung cancer remains the most prevalent maligna...\n",
      "9     Improvements in cancer control have led to a d...\n",
      "10    Non-small cell lung cancer (NSCLC) is the lead...\n",
      "11    Multiple microRNAs encapsulated in extracellul...\n",
      "12    Cisplatin resistance is common in nonâ€‘small ce...\n",
      "13    Cancer will affect more than one in three U.S....\n",
      "14    Thyroid cancer is the most prevalent form of e...\n",
      "15    With significant advancements in the study of ...\n",
      "16    Dysregulation of lung tissue collagen level pl...\n",
      "17    Primary gastric small cell carcinoma (GSCC) is...\n",
      "18    The primary goal of this research is to develo...\n",
      "19    Lung cancer is the most common type of cancer,...\n",
      "20    Several tyrosine kinase receptors inhibitors (...\n",
      "21    India is a vast and diverse country with exist...\n",
      "22    Radiotherapy (RT) is one of the primary treatm...\n",
      "23    Lung Cancer remains one of the leading causes ...\n",
      "24    Lung squamous cell carcinoma (LUSC) is the sec...\n",
      "25    Primary hepatoid adenocarcinoma of the lung (H...\n",
      "26    This study was designed to investigate the cli...\n",
      "27    Human epidermal growth factor receptor 2 (HER2...\n",
      "28    Lung cancer remains a leading cause of cancer-...\n",
      "29    To evaluate the prognostic impact of neutrophi...\n",
      "Name: Abstract, dtype: object\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8608172003114eb3bdd8523f094527f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd8a83d97c744329052d167f65c839b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b62859196e3453d94e717629357c2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "c:\\Users\\Almond\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at C:\\Users\\Almond/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Almond/.cache\\huggingface\\hub\\models--gpt2\\snapshots\\607a30d783dfa663caf39e06633721c8d4cfcd7e\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "c:\\Users\\Almond\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 5\n",
      "  Number of trainable parameters = 124440576\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6adcb228d34840ee8abf3bd057b50911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0115949f0a4a1d9095afd484e06b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 81.71862030029297, 'eval_runtime': 152.9083, 'eval_samples_per_second': 0.033, 'eval_steps_per_second': 0.007, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ed7fce38d04a20add5e2daaff29020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 75.56976318359375, 'eval_runtime': 140.3295, 'eval_samples_per_second': 0.036, 'eval_steps_per_second': 0.007, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178030fbe5b24ce3a5c5e53e9b129802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 70.93292999267578, 'eval_runtime': 151.8005, 'eval_samples_per_second': 0.033, 'eval_steps_per_second': 0.007, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f9ca7d41a244e1b4ac9b9304543840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 67.749755859375, 'eval_runtime': 147.9502, 'eval_samples_per_second': 0.034, 'eval_steps_per_second': 0.007, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dcbef6d80074b2bac0fc867a19b14cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 66.13140869140625, 'eval_runtime': 147.3047, 'eval_samples_per_second': 0.034, 'eval_steps_per_second': 0.007, 'epoch': 1.0}\n",
      "{'train_runtime': 2651.6692, 'train_samples_per_second': 0.008, 'train_steps_per_second': 0.002, 'train_loss': 72.56763305664063, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c33583cc994e62a4f526d76a4de1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation: {'eval_loss': 59.164268493652344, 'eval_runtime': 146.7834, 'eval_samples_per_second': 0.034, 'eval_steps_per_second': 0.007, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load tokenizer and model for causal language modeling (e.g., GPT-2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('Question_5.csv')\n",
    "\n",
    "# Extract the abstracts\n",
    "abstracts = df['Abstract']\n",
    "abstracts = abstracts[:30]  # For demonstration, limit to 3 abstracts\n",
    "print(abstracts)\n",
    "\n",
    "# Create a dataset from the abstracts\n",
    "dataset = Dataset.from_dict({\"text\": abstracts})\n",
    "\n",
    "# Define a preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "# Ensure the input_ids and labels are in integer form (LongTensor)\n",
    "tokenized_dataset = tokenized_dataset.map(lambda x: {\"input_ids\": x[\"input_ids\"].long(), \"labels\": x[\"labels\"].long()})\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1,  # Evaluate every step for demonstration\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_size = 20\n",
    "val_size = 5\n",
    "test_size = 5\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    tokenized_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Ensure the model is in full-precision mode\n",
    "model = model.float()\n",
    "\n",
    "# Update Trainer with train and validation sets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,    # Training set\n",
    "    eval_dataset=val_dataset,       # Validation set for tuning during training\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(\"Test set evaluation:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x000001C355A19B10>\n",
      "Lung cancer is the- of a l g h i n e s,\n",
      ".\n",
      "\n",
      " \" I m o u r f an t my p k y it v c in q b d â€¦ me 7 w'z!\n",
      "Lung cancer is the-\n",
      " I is the I-\n",
      "\n",
      "The G g h i\n",
      "' n a s l e c r o p u w. ''\n",
      "Now PC A H am pc m t, mod P k v in pi f d _ b z x y ï¿½ ï¿½ Â§ - âˆ’ 1 2 3 4 5 6 7\n",
      "Lung cancer is how all the world has been done in a way and I have seen what about that thing had come to.\n",
      " And said you could do it with your, would be like or might show this just of there should not at whatever is the most kind where we are going for on if which m n 1 f\n",
      "Lung cancer is the. for a and I about s n d h e p o v i l g,\n",
      "\n",
      "Lung cancer is a an  in the body which is an Â  m andÂ  is ____., - p n t u v x y z\n",
      " \\ I i e l r s -- -- - -- -- \\ ---- / ( )... \" _ w g h o c f q d j k -- _ --, -- / -- ( -- ) --. / ( / )\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)\n",
    "\n",
    "prompt = \"Lung cancer is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  \n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_length=128,\n",
    "    num_return_sequences=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
